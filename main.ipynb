{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeBxXtUZGCEXOo4JBVldFP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmvufTcEV7rI"
      },
      "outputs": [],
      "source": [
        "# working on fixing bugs\n",
        "# **Section 1: Set Up**\n",
        "#------Imports--------#\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import pickle\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "import functions_framework\n",
        "from google.cloud import storage\n",
        "import io\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "GCS_BUCKET_NAME = os.environ.get('GCS_BUCKET_NAME', 'solar_system_bucket')\n",
        "GCS_FILE_EXTENSION = \".csv\"\n",
        "\n",
        "####################################################################\n",
        "# Read CSV from cloud function\n",
        "def load_data_from_gcs(GCS_BUCKET_NAME, GCS_EXACT_FILE_NAME):\n",
        "    \"\"\"\n",
        "    Downloads a specific CSV file from the specified GCS bucket\n",
        "    and reads it into a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load data from gs://{GCS_BUCKET_NAME}/{GCS_EXACT_FILE_NAME}\")\n",
        "\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(GCS_EXACT_FILE_NAME)\n",
        "\n",
        "        if not blob.exists():\n",
        "            print(f\"File not found: gs://{GCS_BUCKET_NAME}/{GCS_EXACT_FILE_NAME}\")\n",
        "            return pd.DataFrame() # Return empty dataframe if file doesn't exist\n",
        "\n",
        "        data_buffer = io.BytesIO()\n",
        "        blob.download_to_file(data_buffer)\n",
        "        data_buffer.seek(0)\n",
        "        df = pd.read_csv(data_buffer)\n",
        "        print(f\"Successfully loaded {len(df)} rows from GCS into DataFrame.\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"Error loading data from GCS: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return pd.DataFrame({'Error': [f\"Failed to load data: {e}\"]}, index=[0])\n",
        "\n",
        "# This function is now triggered by a Pub/Sub message, not HTTP\n",
        "@functions_framework.cloud_event\n",
        "def process_data_pipeline(cloud_event):\n",
        "    \"\"\"\n",
        "    This function contains the actual data processing logic. It is triggered by a Pub/Sub event.\n",
        "    \"\"\"\n",
        "    print(\"Pub/Sub trigger received. Starting data pipeline...\")\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "    actual_gcs_bucket_name = GCS_BUCKET_NAME\n",
        "\n",
        "    ##########################################################\n",
        "    #EODHD nasdaq_df API\n",
        "    API_KEY = '68433aff09ea73.10710364'\n",
        "    EXCHANGE = 'NASDAQ'\n",
        "    DAYS_BACK = 180\n",
        "    MAX_CALLS_PER_RUN = 200\n",
        "    SECONDS_BETWEEN_CALLS = 0\n",
        "\n",
        "    today = datetime.utcnow().date()\n",
        "    dates = [today - timedelta(days=i) for i in range(DAYS_BACK)]\n",
        "    dates = sorted([d for d in dates if d.weekday() < 5])\n",
        "\n",
        "    downloaded_dates = set()\n",
        "    nasdaq_blob_name = 'nasdaq_df.csv'\n",
        "    nasdaq_blob = bucket.blob(nasdaq_blob_name)\n",
        "    df_existing = pd.DataFrame()\n",
        "\n",
        "    if nasdaq_blob.exists():\n",
        "        df_existing = load_data_from_gcs(GCS_BUCKET_NAME, nasdaq_blob_name)\n",
        "        if not df_existing.empty and 'date' in df_existing.columns:\n",
        "            df_existing['date'] = pd.to_datetime(df_existing['date']).dt.date\n",
        "            downloaded_dates = set(df_existing['date'])\n",
        "\n",
        "    pending_dates = [d for d in dates if d not in downloaded_dates]\n",
        "    all_data = []\n",
        "\n",
        "    dates_to_fetch = pending_dates\n",
        "    if not dates_to_fetch and not downloaded_dates:\n",
        "        dates_to_fetch = dates[:MAX_CALLS_PER_RUN]\n",
        "    elif len(dates_to_fetch) > MAX_CALLS_PER_RUN:\n",
        "        dates_to_fetch = pending_dates[:MAX_CALLS_PER_RUN]\n",
        "\n",
        "    for i, date in enumerate(tqdm(dates_to_fetch, desc=\"Fetching EODHD data\")):\n",
        "        date_str = date.strftime('%Y-%m-%d')\n",
        "        url = f'https://eodhd.com/api/eod-bulk-last-day/{EXCHANGE}?api_token={API_KEY}&fmt=json&date={date_str}'\n",
        "        print(f\"[{i+1}] Fetching {date_str}...\")\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            day_data = response.json()\n",
        "            for entry in day_data:\n",
        "                all_data.append({\n",
        "                    'date': entry.get('date'), 'ticker': entry.get('code'),\n",
        "                    'open': entry.get('open'), 'high': entry.get('high'),\n",
        "                    'low': entry.get('low'), 'close': entry.get('close'),\n",
        "                    'adjusted_close': entry.get('adjusted_close'), 'volume': entry.get('volume'),\n",
        "                })\n",
        "            if i < len(dates_to_fetch) - 1:\n",
        "                time.sleep(SECONDS_BETWEEN_CALLS)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error on {date_str}: {e}\")\n",
        "\n",
        "    if all_data:\n",
        "        df_new = pd.DataFrame(all_data)\n",
        "        df_new['date'] = pd.to_datetime(df_new['date'])\n",
        "\n",
        "        if not df_existing.empty:\n",
        "            df_existing['date'] = pd.to_datetime(df_existing['date'])\n",
        "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "        else:\n",
        "            df_combined = df_new\n",
        "\n",
        "        csv_string = df_combined.to_csv(index=False)\n",
        "        nasdaq_blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "        print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{nasdaq_blob_name}\")\n",
        "\n",
        "    elif downloaded_dates:\n",
        "         print(\"⚠️ No new data fetched, but existing data found.\")\n",
        "    else:\n",
        "        print(\"⚠️ No new data fetched and no existing data found. Aborting.\")\n",
        "        return\n",
        "\n",
        "    nasdaq_df = load_data_from_gcs(GCS_BUCKET_NAME, nasdaq_blob_name)\n",
        "    if nasdaq_df.empty:\n",
        "        print(\"Could not load nasdaq_df after processing. Aborting.\")\n",
        "        return\n",
        "    nasdaq_df['date'] = pd.to_datetime(nasdaq_df['date'])\n",
        "\n",
        "    #############################################################################\n",
        "    # EODHD screener_data_df API\n",
        "    MIN_MARKET_CAP = 10_000_000_000\n",
        "    RESULTS_PER_PAGE = 500\n",
        "\n",
        "    def get_filtered_nasdaq_stocks(api_key, min_cap, exchange=\"NASDAQ\"):\n",
        "        screener_all_data = []\n",
        "        offset = 0\n",
        "        while True:\n",
        "            url = (\n",
        "                \"https://eodhd.com/api/screener\"\n",
        "                f\"?api_token={api_key}&filters=[[\\\"exchange\\\",\\\"=\\\",\\\"{exchange}\\\"],\"\n",
        "                f\"[\\\"market_capitalization\\\",\\\">=\\\",{min_cap}]]\"\n",
        "                f\"&sort=market_capitalization.desc&limit={RESULTS_PER_PAGE}&offset={offset}&fmt=json\"\n",
        "            )\n",
        "            response = requests.get(url)\n",
        "            result = response.json()\n",
        "            batch = result.get(\"data\", [])\n",
        "            if not batch: break\n",
        "            screener_all_data.extend(batch)\n",
        "            offset += RESULTS_PER_PAGE\n",
        "        return pd.DataFrame(screener_all_data)\n",
        "\n",
        "    screener_data_df = get_filtered_nasdaq_stocks(API_KEY, MIN_MARKET_CAP)\n",
        "    meta_url = f'https://eodhd.com/api/exchange-symbol-list/NASDAQ?api_token={API_KEY}&fmt=json'\n",
        "    meta_df = pd.DataFrame(requests.get(meta_url).json())\n",
        "    common_df = meta_df[meta_df['Type'] == 'Common Stock'].copy()\n",
        "    screener_data_df = pd.merge(screener_data_df, common_df[['Code', 'Country', 'Exchange', 'Currency', 'Type']], left_on='code', right_on='Code', how='inner').drop('Code', axis=1)\n",
        "\n",
        "    temp_nasdaq_sorted = nasdaq_df.sort_values(by=['ticker', 'date']).copy()\n",
        "    temp_nasdaq_sorted['prev_adjusted_close'] = temp_nasdaq_sorted.groupby('ticker')['adjusted_close'].shift(1)\n",
        "    temp_nasdaq_sorted['daily_change'] = ((temp_nasdaq_sorted['adjusted_close'] - temp_nasdaq_sorted['prev_adjusted_close']) / temp_nasdaq_sorted['prev_adjusted_close']) * 100\n",
        "    last_day_data_per_ticker = temp_nasdaq_sorted.groupby('ticker').tail(1).copy()\n",
        "    last_day_changes = last_day_data_per_ticker.set_index('ticker')['daily_change'].to_dict()\n",
        "    screener_data_df['last_day_change'] = screener_data_df['code'].map(last_day_changes)\n",
        "\n",
        "    screener_blob_name = \"screener_data_df.csv\"\n",
        "    screener_blob = bucket.blob(screener_blob_name)\n",
        "    csv_string = screener_data_df.to_csv(index=False)\n",
        "    screener_blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "    print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{screener_blob_name}\")\n",
        "\n",
        "    #################################################################################\n",
        "    ## Filter nasdaq data\n",
        "    filtered_nasdaq_df = nasdaq_df[nasdaq_df['ticker'].isin(screener_data_df[screener_data_df['Type'] == 'Common Stock']['code'])]\n",
        "\n",
        "    filtered_nasdaq_blob_name = \"filtered_nasdaq_df.csv\"\n",
        "    filtered_nasdaq_blob = bucket.blob(filtered_nasdaq_blob_name)\n",
        "    csv_string = filtered_nasdaq_df.to_csv(index=False)\n",
        "    filtered_nasdaq_blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "    print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{filtered_nasdaq_blob_name}\")\n",
        "\n",
        "    ##############################################################################\n",
        "    # Function: Correlation Coeficient (Optimized)\n",
        "    def calculate_lagged_correlation(df, lag_days, range_months):\n",
        "      \"\"\"\n",
        "      Calculates the pairwise spearman correlation coefficient with a lag.\n",
        "      This version is optimized to be much faster by pivoting the data first.\n",
        "      \"\"\"\n",
        "      print(f\"Starting optimized correlation calculation for {range_months} months...\")\n",
        "      end_datetime = datetime.now()\n",
        "      start_datetime = end_datetime - pd.DateOffset(months=range_months)\n",
        "\n",
        "      df['date'] = pd.to_datetime(df['date'])\n",
        "      filtered_df_corr = df[(df['date'] >= start_datetime) & (df['date'] <= end_datetime)].copy()\n",
        "      filtered_df_corr = filtered_df_corr[filtered_df_corr['volume'] > 0]\n",
        "\n",
        "      # Pivot the data to have tickers as columns and dates as the index\n",
        "      # This is the core of the optimization\n",
        "      print(\"Pivoting data for faster processing...\")\n",
        "      pivot_df = filtered_df_corr.pivot(index='date', columns='ticker', values='adjusted_close')\n",
        "\n",
        "      tickers = pivot_df.columns\n",
        "      correlation_matrix = pd.DataFrame(index=tickers, columns=tickers, dtype=float)\n",
        "\n",
        "      # Loop through all pairs of tickers\n",
        "      for ticker_a in tqdm(tickers, desc=f\"Calculating {range_months}-month correlations\"):\n",
        "        for ticker_b in tickers:\n",
        "          if ticker_a != ticker_b:\n",
        "            # Correlate stock A with a lagged version of stock B\n",
        "            # This is much faster because the data is already aligned by the pivot\n",
        "            correlation = pivot_df[ticker_a].corr(pivot_df[ticker_b].shift(lag_days), method='spearman')\n",
        "            correlation_matrix.loc[ticker_a, ticker_b] = correlation\n",
        "\n",
        "      print(f\"Finished correlation calculation for {range_months} months.\")\n",
        "      return correlation_matrix\n",
        "\n",
        "    ############################################################################\n",
        "    ## Run Correlation Function\n",
        "    base_file_name_three = \"three_month_spearman_lagged_correlation.csv\"\n",
        "    base_file_name_six = \"six_month_spearman_lagged_correlation.csv\"\n",
        "    blob_three = bucket.blob(base_file_name_three)\n",
        "    blob_six = bucket.blob(base_file_name_six)\n",
        "\n",
        "    run_correlations = bool(all_data) or not blob_three.exists() or not blob_six.exists()\n",
        "\n",
        "    if run_correlations:\n",
        "        print(\"New data fetched or correlation files missing. Calculating correlations...\")\n",
        "        three_month_spearman_lagged_correlations = calculate_lagged_correlation(filtered_nasdaq_df, lag_days=1, range_months=3)\n",
        "        six_month_spearman_lagged_correlations = calculate_lagged_correlation(filtered_nasdaq_df, lag_days=1, range_months=6)\n",
        "\n",
        "        csv_string_three = three_month_spearman_lagged_correlations.to_csv(index=True)\n",
        "        csv_string_six = six_month_spearman_lagged_correlations.to_csv(index=True)\n",
        "        blob_three.upload_from_string(csv_string_three, content_type='text/csv')\n",
        "        blob_six.upload_from_string(csv_string_six, content_type='text/csv')\n",
        "        print(f\"Successfully saved correlation data.\")\n",
        "    else:\n",
        "        print(\"No new data, loading existing correlations.\")\n",
        "        three_month_spearman_lagged_correlations = load_data_from_gcs(GCS_BUCKET_NAME, base_file_name_three)\n",
        "        six_month_spearman_lagged_correlations = load_data_from_gcs(GCS_BUCKET_NAME, base_file_name_six)\n",
        "        if not three_month_spearman_lagged_correlations.empty:\n",
        "          three_month_spearman_lagged_correlations = three_month_spearman_lagged_correlations.set_index(three_month_spearman_lagged_correlations.columns[0])\n",
        "        if not six_month_spearman_lagged_correlations.empty:\n",
        "          six_month_spearman_lagged_correlations = six_month_spearman_lagged_correlations.set_index(six_month_spearman_lagged_correlations.columns[0])\n",
        "\n",
        "    if six_month_spearman_lagged_correlations.empty or three_month_spearman_lagged_correlations.empty:\n",
        "        print(\"Correlation files are still not found or empty after processing. Aborting.\")\n",
        "        return\n",
        "\n",
        "    ##########################################################################\n",
        "    ## Process correlated Data\n",
        "    def process_and_score_stocks(six_month_correlations, three_month_correlations, screener_data_df, source_ticker, min_nodes, max_nodes, threshold_percent):\n",
        "        correlation_df = six_month_correlations.rename_axis('source', axis=0)\n",
        "        grouped_correlation_data = correlation_df.stack().reset_index()\n",
        "        grouped_correlation_data.columns = ['source', 'target', 'six_month_spearman_correlation']\n",
        "        grouped_correlation_data = grouped_correlation_data[(grouped_correlation_data['source'] != grouped_correlation_data['target']) & (grouped_correlation_data['target'] != source_ticker)].copy()\n",
        "        source_connections = grouped_correlation_data[grouped_correlation_data['source'] == source_ticker].copy()\n",
        "        if source_connections.empty: return pd.DataFrame(), pd.DataFrame()\n",
        "        source_connections['three_month_spearman_correlation'] = source_connections.apply(lambda row: three_month_correlations.loc[row['source'], row['target']] if row['source'] in three_month_correlations.index and row['target'] in three_month_correlations.columns else 0, axis=1)\n",
        "        positive_corr_group = source_connections[(source_connections['six_month_spearman_correlation'] > 0) & (source_connections['three_month_spearman_correlation'] > 0)].copy()\n",
        "        if positive_corr_group.empty: return pd.DataFrame(), pd.DataFrame()\n",
        "        screener_cols_to_add = ['code', 'market_capitalization', 'last_day_change']\n",
        "        screener_info = screener_data_df[screener_cols_to_add].rename(columns={'code': 'target'})\n",
        "        positive_corr_group = pd.merge(positive_corr_group, screener_info, on='target', how='left')\n",
        "        positive_corr_group.dropna(subset=['market_capitalization', 'last_day_change'], inplace=True)\n",
        "        if positive_corr_group.empty: return pd.DataFrame(), pd.DataFrame()\n",
        "        epsilon = 1e-9\n",
        "        w_3m = 0.6; w_6m = 0.4\n",
        "        positive_corr_group['unified_correlation'] = (w_3m * positive_corr_group['three_month_spearman_correlation'] + w_6m * positive_corr_group['six_month_spearman_correlation'])\n",
        "        positive_corr_group['Market Cap'] = positive_corr_group['market_capitalization']\n",
        "        source_screener_info = screener_data_df[screener_data_df['code'] == source_ticker]\n",
        "        source_market_cap = source_screener_info['market_capitalization'].iloc[0] if not source_screener_info.empty else epsilon\n",
        "        source_log_cap = np.log(max(source_market_cap, epsilon))\n",
        "        all_market_caps = positive_corr_group['Market Cap'].tolist()\n",
        "        all_market_caps.append(source_market_cap)\n",
        "        log_caps = np.log(pd.Series(all_market_caps).clip(lower=epsilon))\n",
        "        min_log_cap, max_log_cap = log_caps.min(), log_caps.max()\n",
        "        log_cap_range = max_log_cap - min_log_cap\n",
        "        if log_cap_range > 0: positive_corr_group['market_cap_influence'] = np.log(positive_corr_group['Market Cap'].clip(lower=epsilon))\n",
        "        else: positive_corr_group['market_cap_influence'] = 20\n",
        "        correlation_weight_factor = 1.0\n",
        "        positive_corr_group['gravitational_force'] = ((positive_corr_group['unified_correlation'] * correlation_weight_factor) * positive_corr_group['market_cap_influence'])\n",
        "        max_abs_force = positive_corr_group['gravitational_force'].abs().max()\n",
        "        if pd.isna(max_abs_force) or max_abs_force == 0: return pd.DataFrame(), pd.DataFrame()\n",
        "        force_threshold = max_abs_force * threshold_percent\n",
        "        filtered_by_force_threshold = positive_corr_group[positive_corr_group['gravitational_force'].abs() >= force_threshold].copy()\n",
        "        if len(filtered_by_force_threshold) < min_nodes: final_filtered_df = positive_corr_group.sort_values(by='gravitational_force', key=abs, ascending=False).head(min_nodes).copy()\n",
        "        elif len(filtered_by_force_threshold) > max_nodes: final_filtered_df = filtered_by_force_threshold.sort_values(by='gravitational_force', key=abs, ascending=False).head(max_nodes).copy()\n",
        "        else: final_filtered_df = filtered_by_force_threshold.copy()\n",
        "        if final_filtered_df.empty: return pd.DataFrame(), pd.DataFrame()\n",
        "        final_filtered_df['Daily Change'] = final_filtered_df['last_day_change']\n",
        "        final_filtered_df['signed_gravitational_force'] = final_filtered_df.apply(lambda row: row['gravitational_force'] if row['Daily Change'] >= 0 else -row['gravitational_force'], axis=1)\n",
        "        net_gravitational_force = final_filtered_df['signed_gravitational_force'].sum()\n",
        "        max_potential_force = final_filtered_df['market_cap_influence'].sum()\n",
        "        min_corr, max_corr = final_filtered_df['gravitational_force'].min(), final_filtered_df['gravitational_force'].max()\n",
        "        corr_range = max_corr - min_corr if max_corr > min_corr else 1.0\n",
        "        if corr_range > 0: final_filtered_df['Orbital Radius'] = 1 - ((final_filtered_df['gravitational_force'] - min_corr) / corr_range)\n",
        "        else: final_filtered_df['Orbital Radius'] = 0.5\n",
        "        all_caps = pd.concat([final_filtered_df['Market Cap'], pd.Series([source_market_cap])], ignore_index=True)\n",
        "        log_all_caps = np.log(all_caps.clip(lower=epsilon))\n",
        "        min_log_cap = log_all_caps.min(); max_log_cap = log_all_caps.max(); log_cap_range = max_log_cap - min_log_cap\n",
        "        if log_cap_range > 0:\n",
        "            log_df_caps = np.log(final_filtered_df['Market Cap'].clip(lower=epsilon))\n",
        "            final_filtered_df['Planet Radius'] = (log_df_caps - min_log_cap) / log_cap_range\n",
        "        else: final_filtered_df['Planet Radius'] = 0.5\n",
        "        if log_cap_range > 0: source_planet_radius = (source_log_cap - min_log_cap) / log_cap_range\n",
        "        else: source_planet_radius = 0.5\n",
        "        final_filtered_df['gravitational_percent'] = (final_filtered_df['signed_gravitational_force'] / final_filtered_df['gravitational_force'].sum()) * 100\n",
        "        gravitational_impact = (net_gravitational_force / max_potential_force) * 100 if max_potential_force > 0 else 0\n",
        "        source_market_cap_influence = 20 if log_cap_range <= 0 else (source_log_cap)\n",
        "        source_data_df = pd.DataFrame([{'ticker': source_ticker, 'net_gravitational_force': net_gravitational_force, 'max_potential_force': max_potential_force, 'gravitational_impact': gravitational_impact, 'source_market_cap_influence': source_market_cap_influence, 'source_planet_radius': source_planet_radius}])\n",
        "        final_columns = ['source', 'target', 'Daily Change', 'six_month_spearman_correlation', 'three_month_spearman_correlation', 'unified_correlation', 'Orbital Radius', 'Market Cap', 'Planet Radius', 'market_cap_influence', 'gravitational_force', 'signed_gravitational_force', 'gravitational_percent']\n",
        "        for col in final_columns:\n",
        "            if col not in final_filtered_df.columns: final_filtered_df[col] = np.nan\n",
        "        processed_data_df = final_filtered_df[final_columns].copy()\n",
        "        return processed_data_df, source_data_df\n",
        "\n",
        "    ########################################################################\n",
        "    ##List of Top Predictions\n",
        "    min_nodes = 5; max_nodes = 30; threshold_percent = 0.9\n",
        "    top_gravitational_impacts = []\n",
        "\n",
        "    unified_correlation_df = six_month_spearman_lagged_correlations.copy()\n",
        "\n",
        "    for ticker in tqdm(unified_correlation_df.index, desc=\"Processing tickers for gravitational impact\"):\n",
        "        try:\n",
        "            processed_df, source_data = process_and_score_stocks(six_month_spearman_lagged_correlations, three_month_spearman_lagged_correlations, screener_data_df, ticker, min_nodes, max_nodes, threshold_percent)\n",
        "            if not source_data.empty:\n",
        "                top_gravitational_impacts.append({\n",
        "                    'ticker': ticker,\n",
        "                    'net_gravitational_force': source_data['net_gravitational_force'].iloc[0],\n",
        "                    'max_potential_force': source_data['max_potential_force'].iloc[0],\n",
        "                    'gravitational_impact': source_data['gravitational_impact'].iloc[0]\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing ticker {ticker}: {e}\")\n",
        "\n",
        "    gravitational_impact_df = pd.DataFrame(top_gravitational_impacts)\n",
        "\n",
        "    impact_blob_name = \"gravitational_impact_df.csv\"\n",
        "    impact_blob = bucket.blob(impact_blob_name)\n",
        "    csv_string = gravitational_impact_df.to_csv(index=False)\n",
        "    impact_blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "    print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{impact_blob_name}\")\n",
        "    print('Data pipeline finished successfully.')\n",
        "\n",
        "    return \"Pipeline finished.\"\n"
      ]
    }
  ]
}