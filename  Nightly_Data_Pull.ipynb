{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Risskr/Stock-App/blob/Production_v2/%20Nightly_Data_Pull.ipynb",
      "authorship_tag": "ABX9TyPf6zTmFnTbkSjy8BJfqeVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23b73b24a51b4b49b28eaa817dd559d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a978e52762a4d9fad6435253f5aa6e8",
              "IPY_MODEL_7b0f1fa597ae48e1bba09b686542e8e2",
              "IPY_MODEL_b469aed717314f74a59a767361cca18b"
            ],
            "layout": "IPY_MODEL_a25549526eff48489c7a3690a91b464b"
          }
        },
        "6a978e52762a4d9fad6435253f5aa6e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcabfd64d26e4517935046d6826feecf",
            "placeholder": "​",
            "style": "IPY_MODEL_dfdb98c0606145879e23df889c471d05",
            "value": "Processing tickers for gravitational impact: 100%"
          }
        },
        "7b0f1fa597ae48e1bba09b686542e8e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47add0ddca7d4361bfed28d102d549df",
            "max": 262,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae81cae85c684cf1b2bad60ca9851b51",
            "value": 262
          }
        },
        "b469aed717314f74a59a767361cca18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_861c572f18b74971861141d5fb6de501",
            "placeholder": "​",
            "style": "IPY_MODEL_8bd8de4bab654800bbfb304c9277ac05",
            "value": " 262/262 [00:15&lt;00:00, 18.97it/s]"
          }
        },
        "a25549526eff48489c7a3690a91b464b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcabfd64d26e4517935046d6826feecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfdb98c0606145879e23df889c471d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47add0ddca7d4361bfed28d102d549df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae81cae85c684cf1b2bad60ca9851b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "861c572f18b74971861141d5fb6de501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd8de4bab654800bbfb304c9277ac05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Risskr/Stock-App/blob/Production_v2/%20Nightly_Data_Pull.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Set Up**"
      ],
      "metadata": {
        "id": "lUllr9P163K4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chmLOpMh6xGx"
      },
      "outputs": [],
      "source": [
        "#------Imports--------#\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import pickle\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "\n",
        "#-------Froms-------#\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Cloud Setup\n",
        "\n",
        "import functions_framework # Required for all Cloud Functions\n",
        "import pandas as pd      # For creating and manipulating DataFrames\n",
        "from google.cloud import storage # To interact with Google Cloud Storage\n",
        "import os                # To read environment variables\n",
        "import datetime          # To add a timestamp to our file name\n",
        "\n",
        "# --- CONFIGURATION (IMPORTANT: EDIT THIS LINE!) ---\n",
        "# Replace 'YOUR_ACTUAL_GCS_BUCKET_NAME' with the name of your GCS bucket.\n",
        "# Example: 'chris-financial-data-bucket-123'\n",
        "GCS_BUCKET_NAME = os.environ.get('GCS_BUCKET_NAME', 'solar_system_bucket')\n",
        "# The prefix for your data file. A timestamp will be added to it.\n",
        "GCS_FILE_PREFIX = os.environ.get('GCS_FILE_PREFIX', 'solar_system_')\n",
        "GCS_FILE_EXTENSION = \".csv\" # We are now using CSV!\n",
        "# --- SAVE DATAFRAME TO GOOGLE CLOUD STORAGE AS CSV ---\n",
        "# Define the GCS bucket name and the desired file name in the bucket\n",
        "# These values will come from the environment variables we set during deployment\n",
        "actual_gcs_bucket_name = os.getenv('GCS_BUCKET_NAME') # This comes from --set-env-vars\n",
        "actual_gcs_file_prefix = os.getenv('GCS_FILE_PREFIX') # This comes from --set-env-vars\n",
        "# --- END CONFIGURATION ---"
      ],
      "metadata": {
        "id": "EzFKTp17Im3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EODHD nasdaq_df API"
      ],
      "metadata": {
        "id": "gzVAAB2869u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the last 6 months of EODHD Data is available\n",
        "# Savefile\n",
        "\"\"\"\n",
        "Returns:\n",
        "nasdaq_df:\n",
        "  <class 'pandas.core.frame.DataFrame'>\n",
        "  RangeIndex: 627225 entries, 0 to 627224\n",
        "  Data columns (total 8 columns):\n",
        "  #   Column          Non-Null Count   Dtype\n",
        "  ---  ------          --------------   -----\n",
        "  0   date            627225 non-null  datetime64[ns]\n",
        "  1   ticker          627098 non-null  object\n",
        "  2   open            627225 non-null  float64\n",
        "  3   high            627225 non-null  float64\n",
        "  4   low             627225 non-null  float64\n",
        "  5   close           627225 non-null  float64\n",
        "  6   adjusted_close  627225 non-null  float64\n",
        "  7   volume          627225 non-null  float64\n",
        "\"\"\"\n",
        "#API_KEY = 'demo'  # Replace with your API key if not using demo\n",
        "API_KEY = '68433aff09ea73.10710364'\n",
        "EXCHANGE = 'NASDAQ'\n",
        "DAYS_BACK = 180  # Approx. 6 months\n",
        "# #OLD file Path\n",
        "# SAVE_PATH_NASDAQ = '/content/drive/MyDrive/Colab Notebooks/Production/nasdaq_df.csv'\n",
        "MAX_CALLS_PER_RUN = 200  # Use 1–5 for the free tier\n",
        "SECONDS_BETWEEN_CALLS = 0  # Add delay to be respectful\n",
        "\n",
        "# Generate past 6 months of weekdays\n",
        "today = datetime.utcnow().date()\n",
        "dates = [today - timedelta(days=i) for i in range(DAYS_BACK)]\n",
        "dates = sorted([d for d in dates if d.weekday() < 5])  # Keep only weekdays\n",
        "\n",
        "# Load already-downloaded dates if file exists\n",
        "downloaded_dates = set()\n",
        "if os.path.exists(SAVE_PATH_NASDAQ):\n",
        "    df_existing = pd.read_csv(SAVE_PATH_NASDAQ)\n",
        "    # Ensure the date column is treated as datetime objects\n",
        "    df_existing['date'] = pd.to_datetime(df_existing['date']).dt.date\n",
        "    downloaded_dates = set(df_existing['date'])\n",
        "\n",
        "# Filter to only dates we haven’t downloaded\n",
        "pending_dates = [d for d in dates if d not in downloaded_dates]\n",
        "\n",
        "# Prepare data holder\n",
        "all_data = []\n",
        "\n",
        "# Ensure at least the first page of data is fetched if no data is downloaded\n",
        "dates_to_fetch = pending_dates\n",
        "if not dates_to_fetch and not downloaded_dates:\n",
        "    dates_to_fetch = dates[:MAX_CALLS_PER_RUN]\n",
        "elif len(dates_to_fetch) > MAX_CALLS_PER_RUN:\n",
        "    dates_to_fetch = pending_dates[:MAX_CALLS_PER_RUN]\n",
        "\n",
        "\n",
        "for i, date in enumerate(dates_to_fetch):\n",
        "    date_str = date.strftime('%Y-%m-%d')\n",
        "    url = f'https://eodhd.com/api/eod-bulk-last-day/{EXCHANGE}?api_token={API_KEY}&fmt=json&date={date_str}'\n",
        "    print(f\"[{i+1}] Fetching {date_str}...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        day_data = response.json()\n",
        "\n",
        "        for entry in day_data:\n",
        "            all_data.append({\n",
        "                'date': entry.get('date'),\n",
        "                'ticker': entry.get('code'),\n",
        "                'open': entry.get('open'),\n",
        "                'high': entry.get('high'),\n",
        "                'low': entry.get('low'),\n",
        "                'close': entry.get('close'),\n",
        "                'adjusted_close': entry.get('adjusted_close'),\n",
        "                'volume': entry.get('volume'),\n",
        "            })\n",
        "\n",
        "        # Respect the delay\n",
        "        if i < len(dates_to_fetch) - 1:\n",
        "            time.sleep(SECONDS_BETWEEN_CALLS)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error on {date_str}: {e}\")\n",
        "\n",
        "# Append or save the new data\n",
        "if all_data:\n",
        "    df_new = pd.DataFrame(all_data)\n",
        "    # Ensure the date column in df_new is in datetime format for merging/concatenating\n",
        "    df_new['date'] = pd.to_datetime(df_new['date'])\n",
        "\n",
        "    if os.path.exists(SAVE_PATH_NASDAQ):\n",
        "\n",
        "        #Read CSV\n",
        "        df_existing = pd.read_csv(SAVE_PATH_NASDAQ)\n",
        "        df_existing['date'] = pd.to_datetime(df_existing['date'])\n",
        "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "    else:\n",
        "        df_combined = df_new\n",
        "\n",
        "    #Write to CSV\n",
        "    df_combined.to_csv(SAVE_PATH_NASDAQ, index=False)\n",
        "    print(f\"✅ Data for {len(all_data)} entries added to {SAVE_PATH_NASDAQ}\")\n",
        "\n",
        "elif downloaded_dates:\n",
        "     print(\"⚠️ No new data fetched, but existing data found.\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ No new data fetched and no existing data found.\")\n",
        "\n",
        "# Read CSV\n",
        "nasdaq_df = pd.read_csv(SAVE_PATH_NASDAQ)\n",
        "\n",
        "# Save the latest date in YYYYMMDD format\n",
        "nasdaq_df['date'] = pd.to_datetime(nasdaq_df['date'])\n",
        "latest_date_nasdaq_data = nasdaq_df['date'].max().strftime('%Y%m%d')\n",
        "\n",
        "\n",
        "#----------Save to Cloud Storage-----------\n",
        "#------Change-------#\n",
        "# Let's use your desired file name for the base, e.g., 'nasdaq_df'\n",
        "base_file_name = \"nasdaq_df\" # You can make this an env var too if you want!\n",
        "gcs_file_name = f\"{base_file_name}_{GCS_FILE_EXTENSION}\"\n",
        "print(f\"Preparing to save data to GCS: gs://{actual_gcs_bucket_name}/{gcs_file_name}\")\n",
        "# Step 1: Get a GCS client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(actual_gcs_bucket_name)\n",
        "blob = bucket.blob(gcs_file_name) # Define the \"file\" (blob) in your bucket\n",
        "#------Change--------#\n",
        "# Step 2: Convert your DataFrame to a CSV string in memory\n",
        "# This is key! We're not writing to a local file, but to a string variable.\n",
        "csv_string = nasdaq_df.to_csv(index=False) # index=False is good, as you had it!\n",
        "# Step 3: Upload the CSV string to GCS\n",
        "blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{gcs_file_name}\")\n",
        "#----END File save-------"
      ],
      "metadata": {
        "id": "rN6dQvNv7BC3",
        "outputId": "b57aa611-0593-4c1d-c2fa-35c9c212bb8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No new data fetched, but existing data found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EODHD screener_data_df API"
      ],
      "metadata": {
        "id": "Q9ve5P7i7Fvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of Tickers with a min market cap and a list of common stocks\n",
        "# Savefile\n",
        "\"\"\"\n",
        "Returns:\n",
        "screener_data_df:\n",
        "  <class 'pandas.core.frame.DataFrame'>\n",
        "  RangeIndex: 262 entries, 0 to 261\n",
        "  Data columns (total 22 columns):\n",
        " #   Column                 Non-Null Count  Dtype\n",
        "  ---  ------                 --------------  -----\n",
        " 0   code                   262 non-null    object\n",
        " 1   name                   262 non-null    object\n",
        " 2   last_day_data_date     262 non-null    object\n",
        " 3   adjusted_close         262 non-null    float64\n",
        " 4   refund_1d              262 non-null    float64\n",
        " 5   refund_1d_p              262 non-null    float64\n",
        " 6   refund_5d              262 non-null    float64\n",
        " 7   refund_5d_p            262 non-null    float64\n",
        " 8   exchange               262 non-null    object\n",
        " 9   currency_symbol        262 non-null    object\n",
        " 10  market_capitalization  262 non-null    int64\n",
        " 11  earnings_share         262 non-null    float64\n",
        " 12  dividend_yield         145 non-null    float64\n",
        " 13  sector                 262 non-null    object\n",
        " 14  industry               262 non-null    object\n",
        " 15  avgvol_1d              262 non-null    int64\n",
        " 16  avgvol_200d            262 non-null    float64\n",
        " 17  Country                262 non-null    object\n",
        " 18  Exchange               262 non-null    object\n",
        " 19  Currency               262 non-null    object\n",
        " 20  Type                   262 non-null    object\n",
        " 21  last_day_change        262 non-null    float64\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "API_KEY = '68433aff09ea73.10710364'  # Replace with your EODHD key\n",
        "MIN_MARKET_CAP = 10_000_000_000  # Changeable: $1B, $10B, etc.\n",
        "EXCHANGE = 'NASDAQ'\n",
        "RESULTS_PER_PAGE = 500  # Max per EODHD API\n",
        "SAVE_PATH_SCREENER = '/content/drive/MyDrive/Colab Notebooks/Production/screener_data_df.csv'\n",
        "# ------------------------------------------------\n",
        "\n",
        "def get_filtered_nasdaq_stocks(api_key, min_cap, exchange=\"NASDAQ\"):\n",
        "    all_data = []\n",
        "    offset = 0\n",
        "\n",
        "    while True:\n",
        "        url = (\n",
        "            \"https://eodhd.com/api/screener\"\n",
        "            f\"?api_token={api_key}\"\n",
        "            f\"&filters=[\"\n",
        "            f'[\"exchange\",\"=\",\"{exchange}\"],'\n",
        "            #f'[\"type\",\"=\",\"Common Stock\"],'\n",
        "            f'[\"market_capitalization\",\">=\",{min_cap}]'\n",
        "            f\"]\"\n",
        "            f\"&sort=market_capitalization.desc\"\n",
        "            f\"&limit={RESULTS_PER_PAGE}&offset={offset}&fmt=json\"\n",
        "        )\n",
        "\n",
        "        response = requests.get(url)\n",
        "        result = response.json()\n",
        "        batch = result.get(\"data\", [])\n",
        "\n",
        "        if not batch:\n",
        "            break\n",
        "\n",
        "        all_data.extend(batch)\n",
        "        offset += RESULTS_PER_PAGE\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Run filter to get screener_data_df\n",
        "screener_data_df = get_filtered_nasdaq_stocks(API_KEY, MIN_MARKET_CAP)\n",
        "\n",
        "# Get exchange symbol list with type\n",
        "meta_url = f'https://eodhd.com/api/exchange-symbol-list/NASDAQ?api_token={API_KEY}&fmt=json'\n",
        "meta_df = pd.DataFrame(requests.get(meta_url).json())\n",
        "common_df = meta_df[meta_df['Type'] == 'Common Stock'].copy() # Add .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# Combine screener_data_df with relevant columns from common_df\n",
        "screener_data_df = pd.merge(\n",
        "    screener_data_df,\n",
        "    common_df[['Code', 'Country', 'Exchange', 'Currency', 'Type']],\n",
        "    left_on='code',\n",
        "    right_on='Code',\n",
        "    how='inner'\n",
        ").drop('Code', axis=1)\n",
        "\n",
        "# --- Calculate Daily Change ---\n",
        "# Ensure 'date' column is datetime and sort by date\n",
        "nasdaq_df['date'] = pd.to_datetime(nasdaq_df['date'])\n",
        "temp_nasdaq_sorted = nasdaq_df.sort_values(by=['ticker', 'date']).copy()\n",
        "\n",
        "# Calculate the previous day's adjusted close for each ticker\n",
        "temp_nasdaq_sorted['prev_adjusted_close'] = temp_nasdaq_sorted.groupby('ticker')['adjusted_close'].shift(1)\n",
        "\n",
        "# Calculate the daily change as a percentage\n",
        "temp_nasdaq_sorted['daily_change'] = ((temp_nasdaq_sorted['adjusted_close'] - temp_nasdaq_sorted['prev_adjusted_close']) / temp_nasdaq_sorted['prev_adjusted_close']) * 100\n",
        "\n",
        "# Get the last day's data for each ticker\n",
        "last_day_data_per_ticker = temp_nasdaq_sorted.groupby('ticker').tail(1).copy()\n",
        "\n",
        "# Create a dictionary of the last day's change for each ticker\n",
        "last_day_changes = last_day_data_per_ticker.set_index('ticker')['daily_change'].to_dict()\n",
        "\n",
        "# Add the last day change to the screener_data_df\n",
        "screener_data_df['last_day_change'] = screener_data_df['code'].map(last_day_changes)\n",
        "\n",
        "# # Save screener data to csv\n",
        "# screener_data_df.to_csv(SAVE_PATH_SCREENER, index=False)\n",
        "\n",
        "#----------Save to Cloud Storage-----------\n",
        "#------Change-------#\n",
        "# Let's use your desired file name for the base, e.g., 'nasdaq_df'\n",
        "base_file_name = \"screener_data_df\" # You can make this an env var too if you want!\n",
        "gcs_file_name = f\"{base_file_name}_{GCS_FILE_EXTENSION}\"\n",
        "print(f\"Preparing to save data to GCS: gs://{actual_gcs_bucket_name}/{gcs_file_name}\")\n",
        "# Step 1: Get a GCS client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(actual_gcs_bucket_name)\n",
        "blob = bucket.blob(gcs_file_name) # Define the \"file\" (blob) in your bucket\n",
        "#------Change--------#\n",
        "# Step 2: Convert your DataFrame to a CSV string in memory\n",
        "# This is key! We're not writing to a local file, but to a string variable.\n",
        "csv_string = screener_data_df.to_csv(index=False) # index=False is good, as you had it!\n",
        "# Step 3: Upload the CSV string to GCS\n",
        "blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{gcs_file_name}\")\n",
        "#----END File save-----"
      ],
      "metadata": {
        "id": "ZgF4ridt7Idy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter nasdaq data for screener_data_df and type: common"
      ],
      "metadata": {
        "id": "AOQEXPo-7Lsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to filter the Nasdaq stocks to only include tickers that are part of the common_df and the min_market_cap_df. Same this df as a new variable\n",
        "# # Old Save File\n",
        "# SAVE_PATH_FILTERED_NASDAQ = '/content/drive/MyDrive/Colab Notebooks/Production/filtered_nasdaq_df.csv'\n",
        "\n",
        "# Load your EOD data\n",
        "from datetime import datetime, timedelta\n",
        "# nasdaq_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/nasdaq_bulk_eod.csv\")\n",
        "nasdaq_df['date'] = pd.to_datetime(nasdaq_df['date'])\n",
        "\n",
        "# Now filter nasdaq_df using the combined dataframe and type 'Common Stock'\n",
        "filtered_nasdaq_df = nasdaq_df[\n",
        "    nasdaq_df['ticker'].isin(screener_data_df[screener_data_df['Type'] == 'Common Stock']['code'])\n",
        "]\n",
        "\n",
        "# Save filtered nasdaq data to csv\n",
        "filtered_nasdaq_df.to_csv(SAVE_PATH_FILTERED_NASDAQ, index=False)\n",
        "\n",
        "#----------Save to Cloud Storage-----------\n",
        "# --- SAVE DATAFRAME TO GOOGLE CLOUD STORAGE AS CSV ---\n",
        "# Define the GCS bucket name and the desired file name in the bucket\n",
        "# These values will come from the environment variables we set during deployment\n",
        "actual_gcs_bucket_name = os.getenv('GCS_BUCKET_NAME') # This comes from --set-env-vars\n",
        "actual_gcs_file_prefix = os.getenv('GCS_FILE_PREFIX') # This comes from --set-env-vars\n",
        "\n",
        "#------Change-------#\n",
        "# Let's use your desired file name for the base, e.g., 'nasdaq_df'\n",
        "base_file_name = \"filtered_nasdaq_df\" # You can make this an env var too if you want!\n",
        "gcs_file_name = f\"{base_file_name}_{GCS_FILE_EXTENSION}\"\n",
        "\n",
        "\n",
        "print(f\"Preparing to save data to GCS: gs://{actual_gcs_bucket_name}/{gcs_file_name}\")\n",
        "\n",
        "# Step 1: Get a GCS client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(actual_gcs_bucket_name)\n",
        "blob = bucket.blob(gcs_file_name) # Define the \"file\" (blob) in your bucket\n",
        "\n",
        "#------Change--------#\n",
        "# Step 2: Convert your DataFrame to a CSV string in memory\n",
        "# This is key! We're not writing to a local file, but to a string variable.\n",
        "csv_string = filtered_nasdaq_df.to_csv(index=False) # index=False is good, as you had it!\n",
        "\n",
        "# Step 3: Upload the CSV string to GCS\n",
        "blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "\n",
        "print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{gcs_file_name}\")"
      ],
      "metadata": {
        "id": "-U5bQBnT7OID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function: Correlation Coeficient for entire stock data set"
      ],
      "metadata": {
        "id": "4B9kMXP-7O4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#return six month and three month spearman correlations for all unique pairs of stocks\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime # Import datetime\n",
        "from tqdm.notebook import tqdm # Import tqdm\n",
        "\n",
        "# ## Function: Correlation Coeficient for entire stock data set\n",
        "def calculate_lagged_correlation(df, lag_days, range_months):\n",
        "  \"\"\"\n",
        "  Calculates the pairwise spearman correlation coefficient between all stocks\n",
        "  in a DataFrame for a specified period with a given lag.\n",
        "  The start date is calculated by subtracting range_months from today's date.\n",
        "\n",
        "  Args:\n",
        "    df: DataFrame with 'date', 'ticker', and 'adjusted_close' columns.\n",
        "    lag_days: The number of days to lag the second stock's data.\n",
        "    range_months: The number of months to include in the analysis period.\n",
        "\n",
        "  Returns:\n",
        "    correlation_matrix: A pandas DataFrame with the following\n",
        "      Index: Tickers\n",
        "      Columns: Tickers\n",
        "      Values: Spearman correlation coefficients between stocks\n",
        "  \"\"\"\n",
        "  # Calculate the end date (today's date)\n",
        "  end_datetime = datetime.now()\n",
        "\n",
        "  # Calculate the start date for the specified period by subtracting range_months\n",
        "  start_datetime = end_datetime - pd.DateOffset(months = range_months)\n",
        "\n",
        "\n",
        "  # Ensure the 'date' column is in datetime format\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "  # Filter the DataFrame for the specified date range\n",
        "  filtered_df = df[(df['date'] >= start_datetime) & (df['date'] <= end_datetime)].copy()\n",
        "\n",
        "  # Filter out rows where volume is 0\n",
        "  filtered_df = filtered_df[filtered_df['volume'] > 0].copy()\n",
        "\n",
        "  # Get unique tickers in the filtered data\n",
        "  tickers = filtered_df['ticker'].unique()\n",
        "\n",
        "  #breakpoint()\n",
        "\n",
        "  # Create an empty DataFrame to store correlation results\n",
        "  correlation_matrix = pd.DataFrame(index=tickers, columns=tickers, dtype=float)\n",
        "\n",
        "  # Iterate through all pairs of tickers with a progress bar\n",
        "  for ticker_a in tqdm(tickers, desc=\"Calculating correlations\"): # Add tqdm here\n",
        "\n",
        "    # Extract data for each ticker and name the series for clarity\n",
        "    stock_a_data = filtered_df[filtered_df['ticker'] == ticker_a].set_index('date')['adjusted_close']\n",
        "    #breakpoint()\n",
        "\n",
        "    for ticker_b in tickers:\n",
        "      if ticker_a != ticker_b:\n",
        "        # Extract data for each ticker and name the series for clarity\n",
        "        stock_b_data = filtered_df[filtered_df['ticker'] == ticker_b].set_index('date')['adjusted_close']\n",
        "        #breakpoint()\n",
        "\n",
        "        # Align the dataframes based on the date index\n",
        "        # Suffixes will be applied to the 'adjusted_close' column name\n",
        "        aligned_data = pd.merge(stock_a_data, stock_b_data,\n",
        "                                left_index=True, right_index=True,\n",
        "                                how='inner', suffixes=('_A', '_B'))\n",
        "\n",
        "        # Apply the lag to stock_b_data, referencing the suffixed column name\n",
        "        lagged_stock_b_data = aligned_data['adjusted_close_B'].shift(lag_days)\n",
        "\n",
        "        #breakpoint()\n",
        "\n",
        "        # Calculate correlation, dropping NaN values\n",
        "        # Using Spearman method and minimum periods\n",
        "        #correlation = aligned_data['adjusted_close_A'].corr(lagged_stock_b_data, method='spearman', min_periods=100)\n",
        "\n",
        "        correlation = aligned_data['adjusted_close_A'].corr(lagged_stock_b_data, method='spearman')\n",
        "\n",
        "\n",
        "        # Store the correlation in the matrix\n",
        "        correlation_matrix.loc[ticker_a, ticker_b] = correlation\n",
        "\n",
        "  return correlation_matrix\n"
      ],
      "metadata": {
        "id": "tAgzqrRI7Rb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run Correlation Function"
      ],
      "metadata": {
        "id": "YE87NRYF7VT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the nasdaq file was updated, then run the correlation function\n",
        "# Save File\n",
        "\"\"\"\n",
        "Returns:\n",
        "  three_month_spearman_lagged_correlations and six_month_spearman_lagged_correlations: A pandas DataFrame with the following\n",
        "    Index: Tickers\n",
        "    Columns: Tickers\n",
        "    Values: Spearman correlation coefficients between stocks\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the file paths in Google Drive using the determined date\n",
        "three_month_file = f'/content/drive/MyDrive/Colab Notebooks/Production/three_month_spearman_lagged_correlation.csv'\n",
        "six_month_file = f'/content/drive/MyDrive/Colab Notebooks/Production/six_month_spearman_lagged_correlation.csv'\n",
        "\n",
        "\n",
        "#----------Save to Cloud Storage-----------\n",
        "# --- SAVE DATAFRAME TO GOOGLE CLOUD STORAGE AS CSV ---\n",
        "# Define the GCS bucket name and the desired file name in the bucket\n",
        "# These values will come from the environment variables we set during deployment\n",
        "actual_gcs_bucket_name = os.getenv('GCS_BUCKET_NAME') # This comes from --set-env-vars\n",
        "\n",
        "#------Change-------#\n",
        "# Let's use your desired file name for the base, e.g., 'nasdaq_df'\n",
        "base_file_name_three = \"three_month_spearman_lagged_correlation\" # You can make this an env var too if you want!\n",
        "base_file_name_six = \"six_month_spearman_lagged_correlation\" # You can make this an env var too if you want!\n",
        "\n",
        "gcs_file_name_three = f\"{base_file_name_three}_{GCS_FILE_EXTENSION}\"\n",
        "gcs_file_name_six = f\"{base_file_name_six}_{GCS_FILE_EXTENSION}\"\n",
        "\n",
        "print(f\"Preparing to save data to GCS: gs://{actual_gcs_bucket_name}/{gcs_file_name_three}\")\n",
        "print(f\"Preparing to save data to GCS: gs://{actual_gcs_bucket_name}/{gcs_file_name_six}\")\n",
        "\n",
        "# Step 1: Get a GCS client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(actual_gcs_bucket_name)\n",
        "blob_three = bucket.blob(gcs_file_name_three) # Define the \"file\" (blob) in your bucket\n",
        "blob_six = bucket.blob(gcs_file_name_six) # Define the \"file\" (blob) in your bucket\n",
        "\n",
        "\n",
        "# Check if new data was fetched in the previous step\n",
        "if all_data:\n",
        "    print(\"New data fetched. Calculating correlations...\")\n",
        "    # Calculate correlations if files don't exist\n",
        "    # Ensure 'filtered_nasdaq_df' is defined from the preceding code\n",
        "    if 'filtered_nasdaq_df' in locals():\n",
        "        three_month_spearman_lagged_correlations = calculate_lagged_correlation(filtered_nasdaq_df, lag_days=1, range_months=3)\n",
        "        six_month_spearman_lagged_correlations = calculate_lagged_correlation(filtered_nasdaq_df, lag_days=1, range_months=6)\n",
        "\n",
        "\n",
        "        # #OLD Save files\n",
        "        # three_month_spearman_lagged_correlations.to_csv(three_month_file)\n",
        "        # six_month_spearman_lagged_correlations.to_csv(six_month_file)\n",
        "\n",
        "        #------Change--------#\n",
        "        # Step 2: Convert your DataFrame to a CSV string in memory\n",
        "        # This is key! We're not writing to a local file, but to a string variable.\n",
        "        csv_string_three = three_month_spearman_lagged_correlations.to_csv(index=False) # index=False is good, as you had it!\n",
        "        csv_string_six = six_month_spearman_lagged_correlations.to_csv(index=False) # index=False is good, as you had it!\n",
        "\n",
        "        # Step 3: Upload the CSV string to GCS\n",
        "        blob.upload_from_string(csv_string_three, content_type='text/csv')\n",
        "        blob.upload_from_string(csv_string_six, content_type='text/csv')\n",
        "        print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{gcs_file_name_three}\")\n",
        "        print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{gcs_file_name_six}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: 'filtered_nasdaq_df' is not defined. Please ensure the preceding code ran correctly.\")\n",
        "else:\n",
        "    # Check if files exist and load them if no new data was fetched\n",
        "    try:\n",
        "        three_month_spearman_lagged_correlations = pd.read_csv(three_month_file, index_col=0)\n",
        "        print(f\"Loaded existing file: {three_month_file}\")\n",
        "        six_month_spearman_lagged_correlations = pd.read_csv(six_month_file, index_col=0)\n",
        "        print(f\"Loaded existing file: {six_month_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"No new data fetched and one or both correlation files not found. Cannot proceed with correlation calculation.\")"
      ],
      "metadata": {
        "id": "tA-r6IVl7Wwn",
        "outputId": "8f64afca-5ef3-47b1-caee-6f33bc8d677d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing file: /content/drive/MyDrive/Colab Notebooks/Production/three_month_spearman_lagged_correlation.csv\n",
            "Loaded existing file: /content/drive/MyDrive/Colab Notebooks/Production/six_month_spearman_lagged_correlation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process correlated Data and get gravitational scores"
      ],
      "metadata": {
        "id": "cBWWhj--8-OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to process stock correlation data, calculate gravitational forces,\n",
        "# and filter connections for visualization based on the force.\n",
        "def process_and_score_stocks(\n",
        "    six_month_correlations,\n",
        "    three_month_correlations,\n",
        "    screener_data_df,\n",
        "    source_ticker,\n",
        "    min_nodes,\n",
        "    max_nodes,\n",
        "    threshold_percent\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes stock correlation data for a specific source ticker.\n",
        "    It filters for positive correlations, computes a dynamic impact score (gravitational_force),\n",
        "    filters connections, and then calculates a final net gravitational force and the\n",
        "    maximum potential force under ideal conditions.\n",
        "\n",
        "    Args:\n",
        "      six_month_correlations: The six-month spearman lagged correlation matrix.\n",
        "      three_month_correlations: The three-month spearman lagged correlation matrix.\n",
        "      screener_data_df: DataFrame with additional stock information.\n",
        "      source_ticker: The ticker symbol for which to process data.\n",
        "      min_nodes: Minimum number of correlated stocks to return.\n",
        "      max_nodes: Maximum number of correlated stocks to return.\n",
        "      threshold_percent: A percentage (0.0 to 1.0) of the maximum force to use as a filter.\n",
        "\n",
        "    Returns:\n",
        "      processed_data_df: A pandas DataFrame with processed data for visualization.\n",
        "      source_data_df: A pandas DataFrame containing the net_gravitational_force,\n",
        "                      max_potential_force, and gravitational_impact for the source ticker,\n",
        "                      along with the source ticker's market cap influence and source_planet_radius.\n",
        "    \"\"\"\n",
        "    # --- Data Unpivoting and Initial Setup ---\n",
        "    # Start with the 6-month correlation data as the base\n",
        "    correlation_df = six_month_correlations.rename_axis('source', axis=0)\n",
        "    grouped_correlation_data = correlation_df.stack().reset_index()\n",
        "    grouped_correlation_data.columns = ['source', 'target', 'six_month_spearman_correlation']\n",
        "\n",
        "    grouped_correlation_data = grouped_correlation_data[\n",
        "        (grouped_correlation_data['source'] != grouped_correlation_data['target']) &\n",
        "        (grouped_correlation_data['target'] != source_ticker)\n",
        "    ].copy()\n",
        "\n",
        "    # --- Filter for the specific source ticker ---\n",
        "    source_connections = grouped_correlation_data[grouped_correlation_data['source'] == source_ticker].copy()\n",
        "    if source_connections.empty:\n",
        "        print(f\"No correlation data found for source ticker {source_ticker}.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # Add 3-month correlation data before filtering\n",
        "    source_connections['three_month_spearman_correlation'] = source_connections.apply(\n",
        "        lambda row: three_month_correlations.loc[row['source'], row['target']] if row['source'] in three_month_correlations.index and row['target'] in three_month_correlations.columns else 0, axis=1\n",
        "    )\n",
        "\n",
        "    # We only care about positively correlated stocks for this model in both 6 and 3 month periods\n",
        "    positive_corr_group = source_connections[\n",
        "        (source_connections['six_month_spearman_correlation'] > 0) &\n",
        "        (source_connections['three_month_spearman_correlation'] > 0)\n",
        "    ].copy()\n",
        "\n",
        "    if positive_corr_group.empty:\n",
        "        print(f\"No positive correlations found for source ticker {source_ticker}.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # --- Enrich Data (before filtering) ---\n",
        "    # Add market data\n",
        "    screener_cols_to_add = ['code', 'market_capitalization', 'last_day_change']\n",
        "    required_screener_cols = ['code', 'market_capitalization', 'last_day_change']\n",
        "    if not all(col in screener_data_df.columns for col in required_screener_cols):\n",
        "        missing = [col for col in required_screener_cols if col not in screener_data_df.columns]\n",
        "        raise ValueError(f\"screener_data_df is missing required columns: {missing}\")\n",
        "\n",
        "    screener_info = screener_data_df[screener_cols_to_add].rename(columns={'code': 'target'})\n",
        "    positive_corr_group = pd.merge(positive_corr_group, screener_info, on='target', how='left')\n",
        "    positive_corr_group.dropna(subset=['market_capitalization', 'last_day_change'], inplace=True)\n",
        "    if positive_corr_group.empty:\n",
        "        print(f\"No valid connections after merging screener data for {source_ticker}.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "\n",
        "    # --- Calculate Dynamic Impact Score (Gravitational Force) ---\n",
        "    epsilon = 1e-9 # Small value to avoid log(0) issues.\n",
        "    # Weights for recency bias\n",
        "    w_3m = 0.6\n",
        "    w_6m = 0.4\n",
        "    # \"unified_correlation\" is a weighted average of recent correlations.\n",
        "    positive_corr_group['unified_correlation'] = (\n",
        "        w_3m * positive_corr_group['three_month_spearman_correlation'] +\n",
        "        w_6m * positive_corr_group['six_month_spearman_correlation']\n",
        "    )\n",
        "\n",
        "    # Calculate a market cap influence score scaled between 0 and 1 for target stocks.\n",
        "    positive_corr_group['Market Cap'] = positive_corr_group['market_capitalization']\n",
        "\n",
        "    # --- Calculate source ticker's market cap and log cap ---\n",
        "    source_screener_info = screener_data_df[screener_data_df['code'] == source_ticker]\n",
        "    source_market_cap = source_screener_info['market_capitalization'].iloc[0] if not source_screener_info.empty and 'market_capitalization' in source_screener_info.columns else epsilon\n",
        "    source_log_cap = np.log(max(source_market_cap, epsilon))\n",
        "\n",
        "\n",
        "    # Calculate log market caps for all relevant tickers (source and targets)\n",
        "    all_market_caps = positive_corr_group['Market Cap'].tolist()\n",
        "    all_market_caps.append(source_market_cap) # Include source market cap\n",
        "\n",
        "    log_caps = np.log(pd.Series(all_market_caps).clip(lower=epsilon))\n",
        "\n",
        "    min_log_cap, max_log_cap = log_caps.min(), log_caps.max()\n",
        "    log_cap_range = max_log_cap - min_log_cap\n",
        "\n",
        "    # Calculate market cap influence for target stocks\n",
        "    if log_cap_range > 0:\n",
        "        positive_corr_group['market_cap_influence'] = np.log(positive_corr_group['Market Cap'].clip(lower=epsilon))\n",
        "    else:\n",
        "        positive_corr_group['market_cap_influence'] = 20 # Neutral value if all caps are the same\n",
        "\n",
        "\n",
        "    # The `gravitational_force` is a product of recent correlation strength and market influence.\n",
        "    # Modified: Increased the influence of unified_correlation by multiplying by a factor\n",
        "    correlation_weight_factor = 1.0 # Factor to increase the influence of unified_correlation\n",
        "    positive_corr_group['gravitational_force'] = (\n",
        "        (positive_corr_group['unified_correlation'] * correlation_weight_factor) * # Multiply unified_correlation by a factor\n",
        "        positive_corr_group['market_cap_influence']\n",
        "    )\n",
        "\n",
        "    # --- Apply Filtering ---\n",
        "    max_abs_force = positive_corr_group['gravitational_force'].abs().max()\n",
        "    if pd.isna(max_abs_force) or max_abs_force == 0:\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    force_threshold = max_abs_force * threshold_percent\n",
        "    filtered_by_force_threshold = positive_corr_group[positive_corr_group['gravitational_force'].abs() >= force_threshold].copy()\n",
        "\n",
        "    # Enforce min/max node constraints\n",
        "    if len(filtered_by_force_threshold) < min_nodes:\n",
        "        final_filtered_df = positive_corr_group.sort_values(by='gravitational_force', key=abs, ascending=False).head(min_nodes).copy()\n",
        "    elif len(filtered_by_force_threshold) > max_nodes:\n",
        "        final_filtered_df = filtered_by_force_threshold.sort_values(by='gravitational_force', key=abs, ascending=False).head(max_nodes).copy()\n",
        "    else:\n",
        "        final_filtered_df = filtered_by_force_threshold.copy()\n",
        "\n",
        "    if final_filtered_df.empty:\n",
        "        print(f\"No connections remained for {source_ticker} after filtering.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # --- Calculate Final Net Force and Visualization Parameters ---\n",
        "    final_filtered_df['Daily Change'] = final_filtered_df['last_day_change']\n",
        "\n",
        "    final_filtered_df['signed_gravitational_force'] = final_filtered_df.apply(\n",
        "        lambda row: row['gravitational_force'] if row['Daily Change'] >= 0 else -row['gravitational_force'],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    net_gravitational_force = final_filtered_df['signed_gravitational_force'].sum()\n",
        "    max_potential_force = final_filtered_df['market_cap_influence'].sum()\n",
        "\n",
        "    # --- Calculate Visualization Parameters ---\n",
        "    min_corr, max_corr = final_filtered_df['gravitational_force'].min(), final_filtered_df['gravitational_force'].max()\n",
        "    corr_range = max_corr - min_corr if max_corr > min_corr else 1.0\n",
        "    # MODIFIED: Reverse the scaling for Orbital Radius\n",
        "    if corr_range > 0:\n",
        "        final_filtered_df['Orbital Radius'] = 1 - ((final_filtered_df['gravitational_force'] - min_corr) / corr_range)\n",
        "    else:\n",
        "        final_filtered_df['Orbital Radius'] = 0.5 # Neutral value if all forces are the same\n",
        "\n",
        "    # -----Calculate Planet Radius------\n",
        "    # Combine all market caps to find the true min and max for normalization\n",
        "    all_caps = pd.concat([\n",
        "        final_filtered_df['Market Cap'],\n",
        "        pd.Series([source_market_cap]) # Make sure source_market_cap is a Series\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    # Calculate the log, clipping to avoid errors with zero\n",
        "    log_all_caps = np.log(all_caps.clip(lower=epsilon))\n",
        "\n",
        "    # Find the min and max from the complete set of data\n",
        "    min_log_cap = log_all_caps.min()\n",
        "    max_log_cap = log_all_caps.max()\n",
        "    log_cap_range = max_log_cap - min_log_cap\n",
        "\n",
        "    # Now, apply the normalization ONLY to the DataFrame's data\n",
        "    # using the min/max from the combined set\n",
        "    if log_cap_range > 0:\n",
        "        # We are calculating log on just the dataframe column now\n",
        "        log_df_caps = np.log(final_filtered_df['Market Cap'].clip(lower=epsilon))\n",
        "        final_filtered_df['Planet Radius'] = (log_df_caps - min_log_cap) / log_cap_range\n",
        "    else:\n",
        "        # If all values are the same, assign a default radius\n",
        "        final_filtered_df['Planet Radius'] = 0.5\n",
        "\n",
        "    # Calculate source_planet_radius using the same min/max log caps from the targets and source.\n",
        "    if log_cap_range > 0:\n",
        "        source_planet_radius = (source_log_cap - min_log_cap) / log_cap_range\n",
        "    else:\n",
        "        source_planet_radius = 0.5 # Neutral value if all caps are the same\n",
        "\n",
        "    # --- Final Cleanup and Column Selection ---\n",
        "    # \"gravitational_percent\" shows the relative % contribution of each stock.\n",
        "    final_filtered_df['gravitational_percent'] = (final_filtered_df['signed_gravitational_force'] / final_filtered_df['gravitational_force'].sum()) * 100\n",
        "\n",
        "    final_columns = [\n",
        "        'source', 'target', 'Daily Change', 'six_month_spearman_correlation',\n",
        "        'three_month_spearman_correlation', 'unified_correlation',\n",
        "        'Orbital Radius', 'Market Cap', 'Planet Radius', 'market_cap_influence',\n",
        "        'gravitational_force', 'signed_gravitational_force', 'gravitational_percent'\n",
        "    ]\n",
        "\n",
        "\n",
        "    gravitational_impact = (net_gravitational_force / max_potential_force) * 100 if max_potential_force > 0 else 0\n",
        "\n",
        "    # Use the same min_log_cap and log_cap_range from target stocks for scaling\n",
        "    source_market_cap_influence = 20 if log_cap_range <= 0 else (source_log_cap)\n",
        "\n",
        "    # Create source_data_df\n",
        "    source_data_df = pd.DataFrame([{\n",
        "        'ticker': source_ticker,\n",
        "        'net_gravitational_force': net_gravitational_force,\n",
        "        'max_potential_force': max_potential_force,\n",
        "        'gravitational_impact': gravitational_impact,\n",
        "        'source_market_cap_influence': source_market_cap_influence, # Add the source influence\n",
        "        'source_planet_radius': source_planet_radius # Add the source planet radius\n",
        "    }])\n",
        "\n",
        "\n",
        "    for col in final_columns:\n",
        "        if col not in final_filtered_df.columns:\n",
        "            final_filtered_df[col] = np.nan\n",
        "\n",
        "    processed_data_df = final_filtered_df[final_columns].copy()\n",
        "\n",
        "    return processed_data_df, source_data_df\n",
        "\n",
        "\n",
        "# ## ---------- MODIFIED: Run App ---------------\n",
        "# min_nodes = 5\n",
        "# max_nodes = 30\n",
        "# threshold_percent = 0.9\n",
        "\n",
        "# # User input Ticker\n",
        "# source_ticker = 'AAPL'\n",
        "\n",
        "# # Process the data for the network diagram\n",
        "# processed_data_df, source_data_df = process_and_score_stocks(\n",
        "#     six_month_spearman_lagged_correlations,\n",
        "#     three_month_spearman_lagged_correlations,\n",
        "#     screener_data_df,\n",
        "#     source_ticker,\n",
        "#     min_nodes,\n",
        "#     max_nodes,\n",
        "#     threshold_percent,\n",
        "#     )\n",
        "\n",
        "# # Extract the scalar values from the source_data_df for plotting\n",
        "# net_gravitational_force = source_data_df['net_gravitational_force'].iloc[0]\n",
        "# max_potential_force = source_data_df['max_potential_force'].iloc[0]\n",
        "# gravitational_impact = source_data_df['gravitational_impact'].iloc[0]\n",
        "# market_cap_influence = source_data_df['source_market_cap_influence'].iloc[0]\n",
        "# source_planet_radius = source_data_df['source_planet_radius'].iloc[0]\n",
        "\n",
        "# print(f\"Net Gravitational Force: {net_gravitational_force:.2f}\")\n",
        "# print(f\"Max Potential Gravitational Force: {max_potential_force:.2f}\")\n",
        "# print(f\"Net Gravitaional Impact: {gravitational_impact:.2f}%\")\n",
        "# print(f\"Source Market Cap Influence: {market_cap_influence}\")\n",
        "# print(f\"Source Planet Radius: {source_planet_radius}\")\n",
        "# print('----------------------------------')\n",
        "# processed_data_df"
      ],
      "metadata": {
        "id": "42uwf2Yl8-5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##List of Top Predictions"
      ],
      "metadata": {
        "id": "G31Y_ApK8SVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SaveFile\n",
        "\n",
        "#Solar System Parameters\n",
        "min_nodes = 5\n",
        "max_nodes = 30\n",
        "threshold_percent = 0.9\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "top_gravitational_impacts = []\n",
        "\n",
        "# Ensure unified_correlation_df is created from six_month_spearman_lagged_correlations\n",
        "# assuming unified_correlation_df is meant to represent the base correlations\n",
        "# based on the usage in the provided code for process_and_score_stocks.\n",
        "# If unified_correlation_df should be different, please adjust this.\n",
        "\n",
        "# Save Path\n",
        "SAVE_PATH_TOP_PREDICTIONS = '/content/drive/MyDrive/Colab Notebooks/Production/top_gravitational_impacts.csv'\n",
        "\n",
        "unified_correlation_df = six_month_spearman_lagged_correlations.copy()\n",
        "\n",
        "\n",
        "# Iterate through each ticker in the unified_correlation_df\n",
        "for ticker in tqdm(unified_correlation_df.index, desc=\"Processing tickers for gravitational impact\"):\n",
        "    try:\n",
        "        # Run each ticker through the process_and_score_stocks function\n",
        "        # Note: The function previously returned processed_data_df and prediction_score.\n",
        "        # It now returns processed_data_df, net_gravitational_force, max_potential_force, gravitational_impact\n",
        "        # We need to unpack the new return values.\n",
        "        processed_df, source_data = process_and_score_stocks(\n",
        "            six_month_spearman_lagged_correlations, # Pass the 6-month correlation\n",
        "            three_month_spearman_lagged_correlations, # Pass the 3-month correlation\n",
        "            screener_data_df,\n",
        "            ticker, # Use the current ticker as source_ticker\n",
        "            min_nodes,\n",
        "            max_nodes,\n",
        "            threshold_percent,\n",
        "        )\n",
        "\n",
        "        # Append the results to the list\n",
        "        # Access the scalar values from the returned source_data DataFrame\n",
        "        if not source_data.empty:\n",
        "            top_gravitational_impacts.append({\n",
        "                'ticker': ticker,\n",
        "                'net_gravitational_force': source_data['net_gravitational_force'].iloc[0],\n",
        "                'max_potential_force': source_data['max_potential_force'].iloc[0],\n",
        "                'gravitational_impact': source_data['gravitational_impact'].iloc[0]\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing ticker {ticker}: {e}\")\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "gravitational_impact_df = pd.DataFrame(top_gravitational_impacts)\n",
        "\n",
        "# # OLD Save File\n",
        "# gravitational_impact_df.to_csv(SAVE_PATH_TOP_PREDICTIONS, index=False)\n",
        "\n",
        "\n",
        "#----------Save to Cloud Storage-----------\n",
        "#------Change-------#\n",
        "# Let's use your desired file name for the base, e.g., 'nasdaq_df'\n",
        "base_file_name = \"gravitational_impact_df\" # You can make this an env var too if you want!\n",
        "gcs_file_name = f\"{base_file_name}_{GCS_FILE_EXTENSION}\"\n",
        "\n",
        "\n",
        "print(f\"Preparing to save data to GCS: gs://{actual_gcs_bucket_name}/{gcs_file_name}\")\n",
        "\n",
        "# Step 1: Get a GCS client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(actual_gcs_bucket_name)\n",
        "blob = bucket.blob(gcs_file_name) # Define the \"file\" (blob) in your bucket\n",
        "\n",
        "#------Change--------#\n",
        "# Step 2: Convert your DataFrame to a CSV string in memory\n",
        "# This is key! We're not writing to a local file, but to a string variable.\n",
        "csv_string = gravitational_impact_df.to_csv(index=False) # index=False is good, as you had it!\n",
        "\n",
        "# Step 3: Upload the CSV string to GCS\n",
        "blob.upload_from_string(csv_string, content_type='text/csv')\n",
        "\n",
        "print(f\"Successfully saved data to gs://{actual_gcs_bucket_name}/{gcs_file_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jtUI1-fm7bzk",
        "outputId": "9921ac7e-2ec7-4607-ba01-e47f25757af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "23b73b24a51b4b49b28eaa817dd559d7",
            "6a978e52762a4d9fad6435253f5aa6e8",
            "7b0f1fa597ae48e1bba09b686542e8e2",
            "b469aed717314f74a59a767361cca18b",
            "a25549526eff48489c7a3690a91b464b",
            "fcabfd64d26e4517935046d6826feecf",
            "dfdb98c0606145879e23df889c471d05",
            "47add0ddca7d4361bfed28d102d549df",
            "ae81cae85c684cf1b2bad60ca9851b51",
            "861c572f18b74971861141d5fb6de501",
            "8bd8de4bab654800bbfb304c9277ac05"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing tickers for gravitational impact:   0%|          | 0/262 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23b73b24a51b4b49b28eaa817dd559d7"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}