{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDHKgtYNciDEJ/iS2iJFYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Risskr/Stock-App/blob/Production_v2/%20Nightly_Data_Pull.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Set Up**"
      ],
      "metadata": {
        "id": "lUllr9P163K4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chmLOpMh6xGx"
      },
      "outputs": [],
      "source": [
        "#------Imports--------#\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import pickle\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "\n",
        "#-------Froms-------#\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EODHD nasdaq_df API"
      ],
      "metadata": {
        "id": "gzVAAB2869u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the last 6 months of EODHD Data is available\n",
        "# Savefile\n",
        "\"\"\"\n",
        "Returns:\n",
        "nasdaq_df:\n",
        "  <class 'pandas.core.frame.DataFrame'>\n",
        "  RangeIndex: 627225 entries, 0 to 627224\n",
        "  Data columns (total 8 columns):\n",
        "  #   Column          Non-Null Count   Dtype\n",
        "  ---  ------          --------------   -----\n",
        "  0   date            627225 non-null  datetime64[ns]\n",
        "  1   ticker          627098 non-null  object\n",
        "  2   open            627225 non-null  float64\n",
        "  3   high            627225 non-null  float64\n",
        "  4   low             627225 non-null  float64\n",
        "  5   close           627225 non-null  float64\n",
        "  6   adjusted_close  627225 non-null  float64\n",
        "  7   volume          627225 non-null  float64\n",
        "\"\"\"\n",
        "#API_KEY = 'demo'  # Replace with your API key if not using demo\n",
        "API_KEY = '68433aff09ea73.10710364'\n",
        "EXCHANGE = 'NASDAQ'\n",
        "DAYS_BACK = 180  # Approx. 6 months\n",
        "SAVE_PATH_NASDAQ = '/content/drive/MyDrive/Colab Notebooks/Production/nasdaq_bulk_eod.csv'\n",
        "MAX_CALLS_PER_RUN = 200  # Use 1–5 for the free tier\n",
        "SECONDS_BETWEEN_CALLS = 0  # Add delay to be respectful\n",
        "\n",
        "# Generate past 6 months of weekdays\n",
        "today = datetime.utcnow().date()\n",
        "dates = [today - timedelta(days=i) for i in range(DAYS_BACK)]\n",
        "dates = sorted([d for d in dates if d.weekday() < 5])  # Keep only weekdays\n",
        "\n",
        "# Load already-downloaded dates if file exists\n",
        "downloaded_dates = set()\n",
        "if os.path.exists(SAVE_PATH_NASDAQ):\n",
        "    df_existing = pd.read_csv(SAVE_PATH_NASDAQ)\n",
        "    # Ensure the date column is treated as datetime objects\n",
        "    df_existing['date'] = pd.to_datetime(df_existing['date']).dt.date\n",
        "    downloaded_dates = set(df_existing['date'])\n",
        "\n",
        "# Filter to only dates we haven’t downloaded\n",
        "pending_dates = [d for d in dates if d not in downloaded_dates]\n",
        "\n",
        "# Prepare data holder\n",
        "all_data = []\n",
        "\n",
        "# Ensure at least the first page of data is fetched if no data is downloaded\n",
        "dates_to_fetch = pending_dates\n",
        "if not dates_to_fetch and not downloaded_dates:\n",
        "    dates_to_fetch = dates[:MAX_CALLS_PER_RUN]\n",
        "elif len(dates_to_fetch) > MAX_CALLS_PER_RUN:\n",
        "    dates_to_fetch = pending_dates[:MAX_CALLS_PER_RUN]\n",
        "\n",
        "\n",
        "for i, date in enumerate(dates_to_fetch):\n",
        "    date_str = date.strftime('%Y-%m-%d')\n",
        "    url = f'https://eodhd.com/api/eod-bulk-last-day/{EXCHANGE}?api_token={API_KEY}&fmt=json&date={date_str}'\n",
        "    print(f\"[{i+1}] Fetching {date_str}...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        day_data = response.json()\n",
        "\n",
        "        for entry in day_data:\n",
        "            all_data.append({\n",
        "                'date': entry.get('date'),\n",
        "                'ticker': entry.get('code'),\n",
        "                'open': entry.get('open'),\n",
        "                'high': entry.get('high'),\n",
        "                'low': entry.get('low'),\n",
        "                'close': entry.get('close'),\n",
        "                'adjusted_close': entry.get('adjusted_close'),\n",
        "                'volume': entry.get('volume'),\n",
        "            })\n",
        "\n",
        "        # Respect the delay\n",
        "        if i < len(dates_to_fetch) - 1:\n",
        "            time.sleep(SECONDS_BETWEEN_CALLS)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error on {date_str}: {e}\")\n",
        "\n",
        "# Append or save the new data\n",
        "if all_data:\n",
        "    df_new = pd.DataFrame(all_data)\n",
        "    # Ensure the date column in df_new is in datetime format for merging/concatenating\n",
        "    df_new['date'] = pd.to_datetime(df_new['date'])\n",
        "\n",
        "    if os.path.exists(SAVE_PATH_NASDAQ):\n",
        "        df_existing = pd.read_csv(SAVE_PATH_NASDAQ)\n",
        "        df_existing['date'] = pd.to_datetime(df_existing['date'])\n",
        "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "    else:\n",
        "        df_combined = df_new\n",
        "\n",
        "    df_combined.to_csv(SAVE_PATH_NASDAQ, index=False)\n",
        "    print(f\"✅ Data for {len(all_data)} entries added to {SAVE_PATH_NASDAQ}\")\n",
        "\n",
        "elif downloaded_dates:\n",
        "     print(\"⚠️ No new data fetched, but existing data found.\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ No new data fetched and no existing data found.\")\n",
        "\n",
        "\n",
        "# Import CSV into code\n",
        "nasdaq_df = pd.read_csv(SAVE_PATH_NASDAQ)\n",
        "\n",
        "# Save the latest date in YYYYMMDD format\n",
        "nasdaq_df['date'] = pd.to_datetime(nasdaq_df['date'])\n",
        "latest_date_nasdaq_data = nasdaq_df['date'].max().strftime('%Y%m%d')"
      ],
      "metadata": {
        "id": "rN6dQvNv7BC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EODHD screener_data_df API"
      ],
      "metadata": {
        "id": "Q9ve5P7i7Fvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of Tickers with a min market cap and a list of common stocks\n",
        "# Savefile\n",
        "\"\"\"\n",
        "Returns:\n",
        "screener_data_df:\n",
        "  <class 'pandas.core.frame.DataFrame'>\n",
        "  RangeIndex: 262 entries, 0 to 261\n",
        "  Data columns (total 22 columns):\n",
        " #   Column                 Non-Null Count  Dtype\n",
        "  ---  ------                 --------------  -----\n",
        " 0   code                   262 non-null    object\n",
        " 1   name                   262 non-null    object\n",
        " 2   last_day_data_date     262 non-null    object\n",
        " 3   adjusted_close         262 non-null    float64\n",
        " 4   refund_1d              262 non-null    float64\n",
        " 5   refund_1d_p              262 non-null    float64\n",
        " 6   refund_5d              262 non-null    float64\n",
        " 7   refund_5d_p            262 non-null    float64\n",
        " 8   exchange               262 non-null    object\n",
        " 9   currency_symbol        262 non-null    object\n",
        " 10  market_capitalization  262 non-null    int64\n",
        " 11  earnings_share         262 non-null    float64\n",
        " 12  dividend_yield         145 non-null    float64\n",
        " 13  sector                 262 non-null    object\n",
        " 14  industry               262 non-null    object\n",
        " 15  avgvol_1d              262 non-null    int64\n",
        " 16  avgvol_200d            262 non-null    float64\n",
        " 17  Country                262 non-null    object\n",
        " 18  Exchange               262 non-null    object\n",
        " 19  Currency               262 non-null    object\n",
        " 20  Type                   262 non-null    object\n",
        " 21  last_day_change        262 non-null    float64\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "API_KEY = '68433aff09ea73.10710364'  # Replace with your EODHD key\n",
        "MIN_MARKET_CAP = 10_000_000_000  # Changeable: $1B, $10B, etc.\n",
        "EXCHANGE = 'NASDAQ'\n",
        "RESULTS_PER_PAGE = 500  # Max per EODHD API\n",
        "SAVE_PATH_SCREENER = '/content/drive/MyDrive/Colab Notebooks/Production/screener_data_df.csv'\n",
        "# ------------------------------------------------\n",
        "\n",
        "def get_filtered_nasdaq_stocks(api_key, min_cap, exchange=\"NASDAQ\"):\n",
        "    all_data = []\n",
        "    offset = 0\n",
        "\n",
        "    while True:\n",
        "        url = (\n",
        "            \"https://eodhd.com/api/screener\"\n",
        "            f\"?api_token={api_key}\"\n",
        "            f\"&filters=[\"\n",
        "            f'[\"exchange\",\"=\",\"{exchange}\"],'\n",
        "            #f'[\"type\",\"=\",\"Common Stock\"],'\n",
        "            f'[\"market_capitalization\",\">=\",{min_cap}]'\n",
        "            f\"]\"\n",
        "            f\"&sort=market_capitalization.desc\"\n",
        "            f\"&limit={RESULTS_PER_PAGE}&offset={offset}&fmt=json\"\n",
        "        )\n",
        "\n",
        "        response = requests.get(url)\n",
        "        result = response.json()\n",
        "        batch = result.get(\"data\", [])\n",
        "\n",
        "        if not batch:\n",
        "            break\n",
        "\n",
        "        all_data.extend(batch)\n",
        "        offset += RESULTS_PER_PAGE\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Run filter to get screener_data_df\n",
        "screener_data_df = get_filtered_nasdaq_stocks(API_KEY, MIN_MARKET_CAP)\n",
        "\n",
        "# Get exchange symbol list with type\n",
        "meta_url = f'https://eodhd.com/api/exchange-symbol-list/NASDAQ?api_token={API_KEY}&fmt=json'\n",
        "meta_df = pd.DataFrame(requests.get(meta_url).json())\n",
        "common_df = meta_df[meta_df['Type'] == 'Common Stock'].copy() # Add .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# Combine screener_data_df with relevant columns from common_df\n",
        "screener_data_df = pd.merge(\n",
        "    screener_data_df,\n",
        "    common_df[['Code', 'Country', 'Exchange', 'Currency', 'Type']],\n",
        "    left_on='code',\n",
        "    right_on='Code',\n",
        "    how='inner'\n",
        ").drop('Code', axis=1)\n",
        "\n",
        "# --- Calculate Daily Change ---\n",
        "# Ensure 'date' column is datetime and sort by date\n",
        "nasdaq_df['date'] = pd.to_datetime(nasdaq_df['date'])\n",
        "temp_nasdaq_sorted = nasdaq_df.sort_values(by=['ticker', 'date']).copy()\n",
        "\n",
        "# Calculate the previous day's adjusted close for each ticker\n",
        "temp_nasdaq_sorted['prev_adjusted_close'] = temp_nasdaq_sorted.groupby('ticker')['adjusted_close'].shift(1)\n",
        "\n",
        "# Calculate the daily change as a percentage\n",
        "temp_nasdaq_sorted['daily_change'] = ((temp_nasdaq_sorted['adjusted_close'] - temp_nasdaq_sorted['prev_adjusted_close']) / temp_nasdaq_sorted['prev_adjusted_close']) * 100\n",
        "\n",
        "# Get the last day's data for each ticker\n",
        "last_day_data_per_ticker = temp_nasdaq_sorted.groupby('ticker').tail(1).copy()\n",
        "\n",
        "# Create a dictionary of the last day's change for each ticker\n",
        "last_day_changes = last_day_data_per_ticker.set_index('ticker')['daily_change'].to_dict()\n",
        "\n",
        "# Add the last day change to the screener_data_df\n",
        "screener_data_df['last_day_change'] = screener_data_df['code'].map(last_day_changes)\n",
        "\n",
        "# Save screener data to csv\n",
        "screener_data_df.to_csv(SAVE_PATH_SCREENER, index=False)"
      ],
      "metadata": {
        "id": "ZgF4ridt7Idy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter nasdaq data for screener_data_df and type: common"
      ],
      "metadata": {
        "id": "AOQEXPo-7Lsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to filter the Nasdaq stocks to only include tickers that are part of the common_df and the min_market_cap_df. Same this df as a new variable\n",
        "# Save File\n",
        "SAVE_PATH_FILTERED_NASDAQ = '/content/drive/MyDrive/Colab Notebooks/Production/filtered_nasdaq_df.csv'\n",
        "\n",
        "# Load your EOD data\n",
        "from datetime import datetime, timedelta\n",
        "# nasdaq_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/nasdaq_bulk_eod.csv\")\n",
        "nasdaq_df['date'] = pd.to_datetime(nasdaq_df['date'])\n",
        "\n",
        "# Now filter nasdaq_df using the combined dataframe and type 'Common Stock'\n",
        "filtered_nasdaq_df = nasdaq_df[\n",
        "    nasdaq_df['ticker'].isin(screener_data_df[screener_data_df['Type'] == 'Common Stock']['code'])\n",
        "]\n",
        "\n",
        "# Save filtered nasdaq data to csv\n",
        "filtered_nasdaq_df.to_csv(SAVE_PATH_FILTERED_NASDAQ, index=False)"
      ],
      "metadata": {
        "id": "-U5bQBnT7OID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function: Correlation Coeficient for entire stock data set"
      ],
      "metadata": {
        "id": "4B9kMXP-7O4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#return six month and three month spearman correlations for all unique pairs of stocks\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime # Import datetime\n",
        "from tqdm.notebook import tqdm # Import tqdm\n",
        "\n",
        "# ## Function: Correlation Coeficient for entire stock data set\n",
        "def calculate_lagged_correlation(df, lag_days, range_months):\n",
        "  \"\"\"\n",
        "  Calculates the pairwise spearman correlation coefficient between all stocks\n",
        "  in a DataFrame for a specified period with a given lag.\n",
        "  The start date is calculated by subtracting range_months from today's date.\n",
        "\n",
        "  Args:\n",
        "    df: DataFrame with 'date', 'ticker', and 'adjusted_close' columns.\n",
        "    lag_days: The number of days to lag the second stock's data.\n",
        "    range_months: The number of months to include in the analysis period.\n",
        "\n",
        "  Returns:\n",
        "    correlation_matrix: A pandas DataFrame with the following\n",
        "      Index: Tickers\n",
        "      Columns: Tickers\n",
        "      Values: Spearman correlation coefficients between stocks\n",
        "  \"\"\"\n",
        "  # Calculate the end date (today's date)\n",
        "  end_datetime = datetime.now()\n",
        "\n",
        "  # Calculate the start date for the specified period by subtracting range_months\n",
        "  start_datetime = end_datetime - pd.DateOffset(months = range_months)\n",
        "\n",
        "\n",
        "  # Ensure the 'date' column is in datetime format\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "  # Filter the DataFrame for the specified date range\n",
        "  filtered_df = df[(df['date'] >= start_datetime) & (df['date'] <= end_datetime)].copy()\n",
        "\n",
        "  # Filter out rows where volume is 0\n",
        "  filtered_df = filtered_df[filtered_df['volume'] > 0].copy()\n",
        "\n",
        "  # Get unique tickers in the filtered data\n",
        "  tickers = filtered_df['ticker'].unique()\n",
        "\n",
        "  #breakpoint()\n",
        "\n",
        "  # Create an empty DataFrame to store correlation results\n",
        "  correlation_matrix = pd.DataFrame(index=tickers, columns=tickers, dtype=float)\n",
        "\n",
        "  # Iterate through all pairs of tickers with a progress bar\n",
        "  for ticker_a in tqdm(tickers, desc=\"Calculating correlations\"): # Add tqdm here\n",
        "\n",
        "    # Extract data for each ticker and name the series for clarity\n",
        "    stock_a_data = filtered_df[filtered_df['ticker'] == ticker_a].set_index('date')['adjusted_close']\n",
        "    #breakpoint()\n",
        "\n",
        "    for ticker_b in tickers:\n",
        "      if ticker_a != ticker_b:\n",
        "        # Extract data for each ticker and name the series for clarity\n",
        "        stock_b_data = filtered_df[filtered_df['ticker'] == ticker_b].set_index('date')['adjusted_close']\n",
        "        #breakpoint()\n",
        "\n",
        "        # Align the dataframes based on the date index\n",
        "        # Suffixes will be applied to the 'adjusted_close' column name\n",
        "        aligned_data = pd.merge(stock_a_data, stock_b_data,\n",
        "                                left_index=True, right_index=True,\n",
        "                                how='inner', suffixes=('_A', '_B'))\n",
        "\n",
        "        # Apply the lag to stock_b_data, referencing the suffixed column name\n",
        "        lagged_stock_b_data = aligned_data['adjusted_close_B'].shift(lag_days)\n",
        "\n",
        "        #breakpoint()\n",
        "\n",
        "        # Calculate correlation, dropping NaN values\n",
        "        # Using Spearman method and minimum periods\n",
        "        #correlation = aligned_data['adjusted_close_A'].corr(lagged_stock_b_data, method='spearman', min_periods=100)\n",
        "\n",
        "        correlation = aligned_data['adjusted_close_A'].corr(lagged_stock_b_data, method='spearman')\n",
        "\n",
        "\n",
        "        # Store the correlation in the matrix\n",
        "        correlation_matrix.loc[ticker_a, ticker_b] = correlation\n",
        "\n",
        "  return correlation_matrix\n"
      ],
      "metadata": {
        "id": "tAgzqrRI7Rb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run Correlation Function"
      ],
      "metadata": {
        "id": "YE87NRYF7VT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the nasdaq file was updated, then run the correlation function\n",
        "# Save File\n",
        "\"\"\"\n",
        "Returns:\n",
        "  three_month_spearman_lagged_correlations and six_month_spearman_lagged_correlations: A pandas DataFrame with the following\n",
        "    Index: Tickers\n",
        "    Columns: Tickers\n",
        "    Values: Spearman correlation coefficients between stocks\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the file paths in Google Drive using the determined date\n",
        "three_month_file = f'/content/drive/MyDrive/Colab Notebooks/Production/three_month_spearman_lagged_correlation.csv'\n",
        "six_month_file = f'/content/drive/MyDrive/Colab Notebooks/Production/six_month_spearman_lagged_correlation.csv'\n",
        "\n",
        "# Check if new data was fetched in the previous step\n",
        "if all_data:\n",
        "    print(\"New data fetched. Calculating correlations...\")\n",
        "    # Calculate correlations if files don't exist\n",
        "    # Ensure 'filtered_nasdaq_df' is defined from the preceding code\n",
        "    if 'filtered_nasdaq_df' in locals():\n",
        "        three_month_spearman_lagged_correlations = calculate_lagged_correlation(filtered_nasdaq_df, lag_days=1, range_months=3)\n",
        "        six_month_spearman_lagged_correlations = calculate_lagged_correlation(filtered_nasdaq_df, lag_days=1, range_months=6)\n",
        "\n",
        "        three_month_spearman_lagged_correlations.to_csv(three_month_file)\n",
        "        print(f\"Saved calculated file: {three_month_file}\")\n",
        "        six_month_spearman_lagged_correlations.to_csv(six_month_file)\n",
        "        print(f\"Saved calculated file: {six_month_file}\")\n",
        "    else:\n",
        "        print(\"Error: 'filtered_nasdaq_df' is not defined. Please ensure the preceding code ran correctly.\")\n",
        "else:\n",
        "    # Check if files exist and load them if no new data was fetched\n",
        "    try:\n",
        "        three_month_spearman_lagged_correlations = pd.read_csv(three_month_file, index_col=0)\n",
        "        print(f\"Loaded existing file: {three_month_file}\")\n",
        "        six_month_spearman_lagged_correlations = pd.read_csv(six_month_file, index_col=0)\n",
        "        print(f\"Loaded existing file: {six_month_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"No new data fetched and one or both correlation files not found. Cannot proceed with correlation calculation.\")"
      ],
      "metadata": {
        "id": "tA-r6IVl7Wwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##List of Top Predictions"
      ],
      "metadata": {
        "id": "cgStiYL87bM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SaveFile\n",
        "\n",
        "import pandas as pd\n",
        "top_gravitational_impacts = []\n",
        "\n",
        "# Ensure unified_correlation_df is created from six_month_spearman_lagged_correlations\n",
        "# assuming unified_correlation_df is meant to represent the base correlations\n",
        "# based on the usage in the provided code for process_and_score_stocks.\n",
        "# If unified_correlation_df should be different, please adjust this.\n",
        "\n",
        "# Save Path\n",
        "SAVE_PATH_TOP_PREDICTIONS = '/content/drive/MyDrive/Colab Notebooks/Production/top_gravitational_impacts.csv'\n",
        "\n",
        "unified_correlation_df = six_month_spearman_lagged_correlations.copy()\n",
        "\n",
        "\n",
        "# Iterate through each ticker in the unified_correlation_df\n",
        "for ticker in tqdm(unified_correlation_df.index, desc=\"Processing tickers for gravitational impact\"):\n",
        "    try:\n",
        "        # Run each ticker through the process_and_score_stocks function\n",
        "        # Note: The function previously returned processed_data_df and prediction_score.\n",
        "        # It now returns processed_data_df, net_gravitational_force, max_potential_force, gravitational_impact\n",
        "        # We need to unpack the new return values.\n",
        "        processed_df, source_data = process_and_score_stocks(\n",
        "            six_month_spearman_lagged_correlations, # Pass the 6-month correlation\n",
        "            three_month_spearman_lagged_correlations, # Pass the 3-month correlation\n",
        "            screener_data_df,\n",
        "            ticker, # Use the current ticker as source_ticker\n",
        "            min_nodes,\n",
        "            max_nodes,\n",
        "            threshold_percent,\n",
        "        )\n",
        "\n",
        "        # Append the results to the list\n",
        "        # Access the scalar values from the returned source_data DataFrame\n",
        "        if not source_data.empty:\n",
        "            top_gravitational_impacts.append({\n",
        "                'ticker': ticker,\n",
        "                'net_gravitational_force': source_data['net_gravitational_force'].iloc[0],\n",
        "                'max_potential_force': source_data['max_potential_force'].iloc[0],\n",
        "                'gravitational_impact': source_data['gravitational_impact'].iloc[0]\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing ticker {ticker}: {e}\")\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "gravitational_impact_df = pd.DataFrame(top_gravitational_impacts)\n",
        "\n",
        "# Save File\n",
        "gravitational_impact_df.to_csv(SAVE_PATH_TOP_PREDICTIONS, index=False)\n",
        "\n",
        "# # Sort by gravitational_impact in descending order for top positive impacts\n",
        "# top_positive_impacts = gravitational_impact_df.sort_values(by='gravitational_impact', ascending=False).head(10).reset_index(drop=True)\n",
        "\n",
        "# # Sort by gravitational_impact in ascending order for top negative impacts\n",
        "# top_negative_impacts = gravitational_impact_df.sort_values(by='gravitational_impact', ascending=True).head(10).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# # Display the tables\n",
        "# print(\"Top 10 Positive Gravitational Impact Tickers:\")\n",
        "# print(top_positive_impacts[['ticker', 'net_gravitational_force', 'max_potential_force', 'gravitational_impact']].to_string(index=False, formatters={'net_gravitational_force': '{:.2f}'.format, 'max_potential_force': '{:.2f}'.format, 'gravitational_impact': '{:.2f}'.format}))\n",
        "\n",
        "# print(\"\\nTop 10 Negative Gravitational Impact Tickers:\")\n",
        "# print(top_negative_impacts[['ticker', 'net_gravitational_force', 'max_potential_force', 'gravitational_impact']].to_string(index=False, formatters={'net_gravitational_force': '{:.2f}'.format, 'max_potential_force': '{:.2f}'.format, 'gravitational_impact': '{:.2f}'.format}))\n",
        "\n"
      ],
      "metadata": {
        "id": "jtUI1-fm7bzk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}