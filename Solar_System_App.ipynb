{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcAwbHkznp1nVESojcBfjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Risskr/Stock-App/blob/Production_v2/Solar_System_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Up"
      ],
      "metadata": {
        "id": "6nSXkI2t5cjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up"
      ],
      "metadata": {
        "id": "ADxmhQwa5e_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read CSVs"
      ],
      "metadata": {
        "id": "1vfCjewE5gOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Import Nasdaq_df CSV\n",
        "#SAVE_PATH_NASDAQ = '/content/drive/MyDrive/Colab Notebooks/nasdaq_bulk_eod.csv'\n",
        "#nasdaq_df = pd.read_csv({SAVE_PATH_NASDAQ})\n",
        "#latest_date_nasdaq_data = nasdaq_df['date'].max().strftime('%Y%m%d')\n",
        "\n",
        "# Import Three Month Correlation CSV\n",
        "three_month_file = f'/content/drive/MyDrive/Colab Notebooks/Production/three_month_spearman_lagged_correlation.csv'\n",
        "three_month_spearman_lagged_correlations = pd.read_csv(three_month_file, index_col=0)\n",
        "\n",
        "# Import Six Month Correlation CSV\n",
        "six_month_file = f'/content/drive/MyDrive/Colab Notebooks/Production/six_month_spearman_lagged_correlation.csv'\n",
        "six_month_spearman_lagged_correlations = pd.read_csv(six_month_file, index_col=0)\n",
        "\n",
        "# Import Screener_data\n",
        "SAVE_PATH_SCREENER = '/content/drive/MyDrive/Colab Notebooks/Production/screener_data_df.csv'\n",
        "screener_data_df = pd.read_csv(SAVE_PATH_SCREENER)\n",
        "\n",
        "# Import Top Predictions\n",
        "SAVE_PATH_TOP_PREDICTIONS = '/content/drive/MyDrive/Colab Notebooks/Production/top_gravitational_impacts.csv'\n",
        "top_predictions_df = pd.read_csv(SAVE_PATH_TOP_PREDICTIONS)\n",
        "\n",
        "#Solar System Parameters\n",
        "min_nodes = 5\n",
        "max_nodes = 30\n",
        "threshold_percent = 0.9\n",
        "\n",
        "# Default source Ticker\n",
        "source_ticker = 'AAPL'\n",
        "\n"
      ],
      "metadata": {
        "id": "Ssh3ryTf5jy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process correlated Data and get gravitational scores"
      ],
      "metadata": {
        "id": "VzYTi0WB5qGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to process stock correlation data, calculate gravitational forces,\n",
        "# and filter connections for visualization based on the force.\n",
        "\n",
        "def process_and_score_stocks(\n",
        "    six_month_correlations,\n",
        "    three_month_correlations,\n",
        "    screener_data_df,\n",
        "    source_ticker,\n",
        "    min_nodes,\n",
        "    max_nodes,\n",
        "    threshold_percent\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes stock correlation data for a specific source ticker.\n",
        "    It filters for positive correlations, computes a dynamic impact score (gravitational_force),\n",
        "    filters connections, and then calculates a final net gravitational force and the\n",
        "    maximum potential force under ideal conditions.\n",
        "\n",
        "    Args:\n",
        "      six_month_correlations: The six-month spearman lagged correlation matrix.\n",
        "      three_month_correlations: The three-month spearman lagged correlation matrix.\n",
        "      screener_data_df: DataFrame with additional stock information.\n",
        "      source_ticker: The ticker symbol for which to process data.\n",
        "      min_nodes: Minimum number of correlated stocks to return.\n",
        "      max_nodes: Maximum number of correlated stocks to return.\n",
        "      threshold_percent: A percentage (0.0 to 1.0) of the maximum force to use as a filter.\n",
        "\n",
        "    Returns:\n",
        "      processed_data_df: A pandas DataFrame with processed data for visualization.\n",
        "      source_data_df: A pandas DataFrame containing the net_gravitational_force,\n",
        "                      max_potential_force, and gravitational_impact for the source ticker,\n",
        "                      along with the source ticker's market cap influence and source_planet_radius.\n",
        "    \"\"\"\n",
        "    # --- Data Unpivoting and Initial Setup ---\n",
        "    # Start with the 6-month correlation data as the base\n",
        "    correlation_df = six_month_correlations.rename_axis('source', axis=0)\n",
        "    grouped_correlation_data = correlation_df.stack().reset_index()\n",
        "    grouped_correlation_data.columns = ['source', 'target', 'six_month_spearman_correlation']\n",
        "\n",
        "    grouped_correlation_data = grouped_correlation_data[\n",
        "        (grouped_correlation_data['source'] != grouped_correlation_data['target']) &\n",
        "        (grouped_correlation_data['target'] != source_ticker)\n",
        "    ].copy()\n",
        "\n",
        "    # --- Filter for the specific source ticker ---\n",
        "    source_connections = grouped_correlation_data[grouped_correlation_data['source'] == source_ticker].copy()\n",
        "    if source_connections.empty:\n",
        "        print(f\"No correlation data found for source ticker {source_ticker}.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # Add 3-month correlation data before filtering\n",
        "    source_connections['three_month_spearman_correlation'] = source_connections.apply(\n",
        "        lambda row: three_month_correlations.loc[row['source'], row['target']] if row['source'] in three_month_correlations.index and row['target'] in three_month_correlations.columns else 0, axis=1\n",
        "    )\n",
        "\n",
        "    # We only care about positively correlated stocks for this model in both 6 and 3 month periods\n",
        "    positive_corr_group = source_connections[\n",
        "        (source_connections['six_month_spearman_correlation'] > 0) &\n",
        "        (source_connections['three_month_spearman_correlation'] > 0)\n",
        "    ].copy()\n",
        "\n",
        "    if positive_corr_group.empty:\n",
        "        print(f\"No positive correlations found for source ticker {source_ticker}.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # --- Enrich Data (before filtering) ---\n",
        "    # Add market data\n",
        "    screener_cols_to_add = ['code', 'market_capitalization', 'last_day_change']\n",
        "    required_screener_cols = ['code', 'market_capitalization', 'last_day_change']\n",
        "    if not all(col in screener_data_df.columns for col in required_screener_cols):\n",
        "        missing = [col for col in required_screener_cols if col not in screener_data_df.columns]\n",
        "        raise ValueError(f\"screener_data_df is missing required columns: {missing}\")\n",
        "\n",
        "    screener_info = screener_data_df[screener_cols_to_add].rename(columns={'code': 'target'})\n",
        "    positive_corr_group = pd.merge(positive_corr_group, screener_info, on='target', how='left')\n",
        "    positive_corr_group.dropna(subset=['market_capitalization', 'last_day_change'], inplace=True)\n",
        "    if positive_corr_group.empty:\n",
        "        print(f\"No valid connections after merging screener data for {source_ticker}.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "\n",
        "    # --- Calculate Dynamic Impact Score (Gravitational Force) ---\n",
        "    epsilon = 1e-9 # Small value to avoid log(0) issues.\n",
        "    # Weights for recency bias\n",
        "    w_3m = 0.6\n",
        "    w_6m = 0.4\n",
        "    # \"unified_correlation\" is a weighted average of recent correlations.\n",
        "    positive_corr_group['unified_correlation'] = (\n",
        "        w_3m * positive_corr_group['three_month_spearman_correlation'] +\n",
        "        w_6m * positive_corr_group['six_month_spearman_correlation']\n",
        "    )\n",
        "\n",
        "    # Calculate a market cap influence score scaled between 0 and 1 for target stocks.\n",
        "    positive_corr_group['Market Cap'] = positive_corr_group['market_capitalization']\n",
        "\n",
        "    # --- Calculate source ticker's market cap and log cap ---\n",
        "    source_screener_info = screener_data_df[screener_data_df['code'] == source_ticker]\n",
        "    source_market_cap = source_screener_info['market_capitalization'].iloc[0] if not source_screener_info.empty and 'market_capitalization' in source_screener_info.columns else epsilon\n",
        "    source_log_cap = np.log(max(source_market_cap, epsilon))\n",
        "\n",
        "\n",
        "    # Calculate log market caps for all relevant tickers (source and targets)\n",
        "    all_market_caps = positive_corr_group['Market Cap'].tolist()\n",
        "    all_market_caps.append(source_market_cap) # Include source market cap\n",
        "\n",
        "    log_caps = np.log(pd.Series(all_market_caps).clip(lower=epsilon))\n",
        "\n",
        "    min_log_cap, max_log_cap = log_caps.min(), log_caps.max()\n",
        "    log_cap_range = max_log_cap - min_log_cap\n",
        "\n",
        "    # Calculate market cap influence for target stocks\n",
        "    if log_cap_range > 0:\n",
        "        positive_corr_group['market_cap_influence'] = np.log(positive_corr_group['Market Cap'].clip(lower=epsilon))\n",
        "    else:\n",
        "        positive_corr_group['market_cap_influence'] = 20 # Neutral value if all caps are the same\n",
        "\n",
        "\n",
        "    # The `gravitational_force` is a product of recent correlation strength and market influence.\n",
        "    # Modified: Increased the influence of unified_correlation by multiplying by a factor\n",
        "    correlation_weight_factor = 1.0 # Factor to increase the influence of unified_correlation\n",
        "    positive_corr_group['gravitational_force'] = (\n",
        "        (positive_corr_group['unified_correlation'] * correlation_weight_factor) * # Multiply unified_correlation by a factor\n",
        "        positive_corr_group['market_cap_influence']\n",
        "    )\n",
        "\n",
        "    # --- Apply Filtering ---\n",
        "    max_abs_force = positive_corr_group['gravitational_force'].abs().max()\n",
        "    if pd.isna(max_abs_force) or max_abs_force == 0:\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    force_threshold = max_abs_force * threshold_percent\n",
        "    filtered_by_force_threshold = positive_corr_group[positive_corr_group['gravitational_force'].abs() >= force_threshold].copy()\n",
        "\n",
        "    # Enforce min/max node constraints\n",
        "    if len(filtered_by_force_threshold) < min_nodes:\n",
        "        final_filtered_df = positive_corr_group.sort_values(by='gravitational_force', key=abs, ascending=False).head(min_nodes).copy()\n",
        "    elif len(filtered_by_force_threshold) > max_nodes:\n",
        "        final_filtered_df = filtered_by_force_threshold.sort_values(by='gravitational_force', key=abs, ascending=False).head(max_nodes).copy()\n",
        "    else:\n",
        "        final_filtered_df = filtered_by_force_threshold.copy()\n",
        "\n",
        "    if final_filtered_df.empty:\n",
        "        print(f\"No connections remained for {source_ticker} after filtering.\")\n",
        "        # Return empty dataframes when no data is found\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # --- Calculate Final Net Force and Visualization Parameters ---\n",
        "    final_filtered_df['Daily Change'] = final_filtered_df['last_day_change']\n",
        "\n",
        "    final_filtered_df['signed_gravitational_force'] = final_filtered_df.apply(\n",
        "        lambda row: row['gravitational_force'] if row['Daily Change'] >= 0 else -row['gravitational_force'],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    net_gravitational_force = final_filtered_df['signed_gravitational_force'].sum()\n",
        "    max_potential_force = final_filtered_df['market_cap_influence'].sum()\n",
        "\n",
        "    # --- Calculate Visualization Parameters ---\n",
        "    min_corr, max_corr = final_filtered_df['gravitational_force'].min(), final_filtered_df['gravitational_force'].max()\n",
        "    corr_range = max_corr - min_corr if max_corr > min_corr else 1.0\n",
        "    # MODIFIED: Reverse the scaling for Orbital Radius\n",
        "    if corr_range > 0:\n",
        "        final_filtered_df['Orbital Radius'] = 1 - ((final_filtered_df['gravitational_force'] - min_corr) / corr_range)\n",
        "    else:\n",
        "        final_filtered_df['Orbital Radius'] = 0.5 # Neutral value if all forces are the same\n",
        "\n",
        "    # -----Calculate Planet Radius------\n",
        "    # Combine all market caps to find the true min and max for normalization\n",
        "    all_caps = pd.concat([\n",
        "        final_filtered_df['Market Cap'],\n",
        "        pd.Series([source_market_cap]) # Make sure source_market_cap is a Series\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    # Calculate the log, clipping to avoid errors with zero\n",
        "    log_all_caps = np.log(all_caps.clip(lower=epsilon))\n",
        "\n",
        "    # Find the min and max from the complete set of data\n",
        "    min_log_cap = log_all_caps.min()\n",
        "    max_log_cap = log_all_caps.max()\n",
        "    log_cap_range = max_log_cap - min_log_cap\n",
        "\n",
        "    # Now, apply the normalization ONLY to the DataFrame's data\n",
        "    # using the min/max from the combined set\n",
        "    if log_cap_range > 0:\n",
        "        # We are calculating log on just the dataframe column now\n",
        "        log_df_caps = np.log(final_filtered_df['Market Cap'].clip(lower=epsilon))\n",
        "        final_filtered_df['Planet Radius'] = (log_df_caps - min_log_cap) / log_cap_range\n",
        "    else:\n",
        "        # If all values are the same, assign a default radius\n",
        "        final_filtered_df['Planet Radius'] = 0.5\n",
        "\n",
        "    # Calculate source_planet_radius using the same min/max log caps from the targets and source.\n",
        "    if log_cap_range > 0:\n",
        "        source_planet_radius = (source_log_cap - min_log_cap) / log_cap_range\n",
        "    else:\n",
        "        source_planet_radius = 0.5 # Neutral value if all caps are the same\n",
        "\n",
        "    # --- Final Cleanup and Column Selection ---\n",
        "    # \"gravitational_percent\" shows the relative % contribution of each stock.\n",
        "    final_filtered_df['gravitational_percent'] = (final_filtered_df['signed_gravitational_force'] / final_filtered_df['gravitational_force'].sum()) * 100\n",
        "\n",
        "    final_columns = [\n",
        "        'source', 'target', 'Daily Change', 'six_month_spearman_correlation',\n",
        "        'three_month_spearman_correlation', 'unified_correlation',\n",
        "        'Orbital Radius', 'Market Cap', 'Planet Radius', 'market_cap_influence',\n",
        "        'gravitational_force', 'signed_gravitational_force', 'gravitational_percent'\n",
        "    ]\n",
        "\n",
        "\n",
        "    gravitational_impact = (net_gravitational_force / max_potential_force) * 100 if max_potential_force > 0 else 0\n",
        "\n",
        "    # Use the same min_log_cap and log_cap_range from target stocks for scaling\n",
        "    source_market_cap_influence = 20 if log_cap_range <= 0 else (source_log_cap)\n",
        "\n",
        "    # Create source_data_df\n",
        "    source_data_df = pd.DataFrame([{\n",
        "        'ticker': source_ticker,\n",
        "        'net_gravitational_force': net_gravitational_force,\n",
        "        'max_potential_force': max_potential_force,\n",
        "        'gravitational_impact': gravitational_impact,\n",
        "        'source_market_cap_influence': source_market_cap_influence, # Add the source influence\n",
        "        'source_planet_radius': source_planet_radius # Add the source planet radius\n",
        "    }])\n",
        "\n",
        "\n",
        "    for col in final_columns:\n",
        "        if col not in final_filtered_df.columns:\n",
        "            final_filtered_df[col] = np.nan\n",
        "\n",
        "    processed_data_df = final_filtered_df[final_columns].copy()\n",
        "\n",
        "    return processed_data_df, source_data_df\n",
        "\n",
        "\n",
        "# Process the data for the network diagram\n",
        "processed_data_df, source_data_df = process_and_score_stocks(\n",
        "    six_month_spearman_lagged_correlations,\n",
        "    three_month_spearman_lagged_correlations,\n",
        "    screener_data_df,\n",
        "    source_ticker,\n",
        "    min_nodes,\n",
        "    max_nodes,\n",
        "    threshold_percent,\n",
        "    )\n",
        "\n",
        "# Extract the scalar values from the source_data_df for plotting\n",
        "net_gravitational_force = source_data_df['net_gravitational_force'].iloc[0]\n",
        "max_potential_force = source_data_df['max_potential_force'].iloc[0]\n",
        "gravitational_impact = source_data_df['gravitational_impact'].iloc[0]\n",
        "market_cap_influence = source_data_df['source_market_cap_influence'].iloc[0]\n",
        "source_planet_radius = source_data_df['source_planet_radius'].iloc[0]\n",
        "\n",
        "# print(f\"Net Gravitational Force: {net_gravitational_force:.2f}\")\n",
        "# print(f\"Max Potential Gravitational Force: {max_potential_force:.2f}\")\n",
        "# print(f\"Net Gravitaional Impact: {gravitational_impact:.2f}%\")\n",
        "# print(f\"Source Market Cap Influence: {market_cap_influence}\")\n",
        "# print(f\"Source Planet Radius: {source_planet_radius}\")\n",
        "# print('----------------------------------')\n",
        "# processed_data_df"
      ],
      "metadata": {
        "id": "Dpxa6mk55rPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##List of Top Predictions"
      ],
      "metadata": {
        "id": "Pgl7Zsb85wSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter Top Predictions for visual aid\n",
        "\n",
        "# Sort by gravitational_impact in descending order for top positive impacts\n",
        "top_positive_impacts = gravitational_impact_df.sort_values(by='gravitational_impact', ascending=False).head(10).reset_index(drop=True)\n",
        "\n",
        "# Sort by gravitational_impact in ascending order for top negative impacts\n",
        "top_negative_impacts = gravitational_impact_df.sort_values(by='gravitational_impact', ascending=True).head(10).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "xD5-ekdQ5xOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Ngrok"
      ],
      "metadata": {
        "id": "ckDQkLjn51LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ngrok Setup\n",
        "\n",
        "#Terminal Command\n",
        "#ngrok config add-authtoken 2yPWVwCXJDsY9JkhLhnrn549R5E_5b2uFuh7R4ER3fLJWLU59\n",
        "\n",
        "# Install necessary libraries (run this cell first)\n",
        "!pip install -q pyngrok\n",
        "\n",
        "# --- Import necessary libraries ---\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Configure Ngrok for public access ---\n",
        "# This part remains the same to create a public URL for your app\n",
        "try:\n",
        "    NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    # Use pyngrok to set the auth token programmatically\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"NGROK_AUTH_TOKEN not found in Colab secrets. Please add it.\")\n",
        "    # You might want to handle this case, e.g., by exiting or using a default token if applicable\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Define the port your Dash app will run on.\n",
        "DASH_PORT = 8051"
      ],
      "metadata": {
        "id": "Yx7Wzqak51pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a public ngrok tunnel to the specified port.\n",
        "print(\"Establishing Ngrok tunnel...\")\n",
        "public_url = ngrok.connect(DASH_PORT)\n",
        "print(f\"ðŸš€ Your Dash app will be accessible at: {public_url}\")"
      ],
      "metadata": {
        "id": "1Wgv7ITC53lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kills all ngrok processes that pyngrok started\n",
        "print(\"Attempting to kill all active ngrok tunnels...\")\n",
        "ngrok.kill()\n",
        "print(\"All ngrok tunnels should now be terminated.\")\n",
        "\n",
        "# Add a short delay to ensure the process is killed\n",
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "uL6QFPKq57a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dash App"
      ],
      "metadata": {
        "id": "xxyC--xF5-6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dash App Setup\n",
        "\n",
        "# Install necessary libraries (run this cell first)\n",
        "!pip install -q dash dash-bootstrap-components plotly\n",
        "\n",
        "# --- Import necessary libraries ---\n",
        "from dash import Dash, html, dcc, Input, Output # We need dcc for interactive components and Input/Output for callbacks\n"
      ],
      "metadata": {
        "id": "8xjOcwMv5_xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a dash app by Chris Parks\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.colors as mcolors\n",
        "from dash import Dash, dcc, html, callback_context\n",
        "from dash.dependencies import Input, Output, State, ALL\n",
        "from dash.exceptions import PreventUpdate\n",
        "import json\n",
        "import base64\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def create_model_image_svg(base_color, subdivisions, texture_spots_count):\n",
        "    \"\"\"\n",
        "    Creates a base64 encoded SVG data URL that is a 2D representation of the 3D low-poly model.\n",
        "    This function generates the geometry, projects it, and simulates flat shading to match the plot.\n",
        "    \"\"\"\n",
        "    # 1. Generate the 3D model's vertices and faces for a unit sphere\n",
        "    # We pass 0 for texture_spots here because we'll draw them separately in the SVG\n",
        "    vertices, faces, _ = create_low_poly_sphere(0, 0, 0, 1, base_color, subdivisions, 0)\n",
        "\n",
        "    # 2. Define a light source for shading the facets\n",
        "    light_source = np.array([-0.5, 0.8, 1.0])\n",
        "    light_source = light_source / np.linalg.norm(light_source)\n",
        "\n",
        "    # 3. Process each face for rendering\n",
        "    face_data = []\n",
        "    for face in faces:\n",
        "        # Get the vertices for the current face\n",
        "        v0, v1, v2 = vertices[face]\n",
        "\n",
        "        # --- Back-face culling: Don't render faces pointing away from the camera ---\n",
        "        # The camera is at (0, 0, z), so we check the z-component of the normal\n",
        "        normal = np.cross(v1 - v0, v2 - v0)\n",
        "        if np.linalg.norm(normal) == 0: continue\n",
        "        normal = normal / np.linalg.norm(normal)\n",
        "        if normal[2] < 0:\n",
        "            continue # This face is on the back of the sphere, so we skip it\n",
        "\n",
        "        # --- Shading: Calculate brightness based on angle to the light source ---\n",
        "        intensity = np.dot(normal, light_source)\n",
        "        # Map intensity to a brightness factor for the color\n",
        "        color_factor = 0.65 + intensity * 0.5\n",
        "        facet_color = darken_color(base_color, color_factor)\n",
        "\n",
        "        # --- Projection: Convert 3D vertex coordinates to 2D SVG coordinates ---\n",
        "        # We scale and shift the (x, y) coordinates to fit in a 100x100 SVG\n",
        "        points_2d_str = \" \".join([f\"{(v[0] * 48) + 50},{(v[1] * -48) + 50}\" for v in [v0, v1, v2]])\n",
        "\n",
        "        # Store the face's z-depth for sorting, so closer faces draw on top\n",
        "        avg_z = (v0[2] + v1[2] + v2[2]) / 3\n",
        "        face_data.append({'z': avg_z, 'points': points_2d_str, 'color': facet_color})\n",
        "\n",
        "    # 4. Sort faces from back to front\n",
        "    face_data.sort(key=lambda f: f['z'])\n",
        "\n",
        "    # 5. Build the SVG polygons from the sorted face data\n",
        "    svg_polygons = \"\".join(f'<polygon points=\"{f[\"points\"]}\" fill=\"{f[\"color\"]}\" />' for f in face_data)\n",
        "\n",
        "    # 6. Add texture spots as random circles\n",
        "    texture_color = darken_color(base_color, 0.7)\n",
        "    svg_texture_spots = \"\"\n",
        "    np.random.seed(sum(ord(c) for c in base_color)) # Seed for consistency\n",
        "    for _ in range(texture_spots_count):\n",
        "        angle = np.random.uniform(0, 2 * np.pi)\n",
        "        radius = np.random.uniform(0, 48)\n",
        "        spot_size = np.random.uniform(4, 9)\n",
        "        cx = 50 + radius * np.cos(angle)\n",
        "        cy = 50 + radius * np.sin(angle)\n",
        "        svg_texture_spots += f'<circle cx=\"{cx}\" cy=\"{cy}\" r=\"{spot_size}\" fill=\"{texture_color}\" opacity=\"0.7\"/>'\n",
        "\n",
        "    # 7. Assemble the final SVG string\n",
        "    svg_string = f'''\n",
        "    <svg width=\"100\" height=\"100\" viewBox=\"0 0 100 100\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "      <defs>\n",
        "        <clipPath id=\"sphereClip\">\n",
        "          <circle cx=\"50\" cy=\"50\" r=\"48\"/>\n",
        "        </clipPath>\n",
        "        <filter id=\"blur-effect\">\n",
        "          <feGaussianBlur in=\"SourceGraphic\" stdDeviation=\"0.7\" />\n",
        "        </filter>\n",
        "      </defs>\n",
        "      <g clip-path=\"url(#sphereClip)\" filter=\"url(#blur-effect)\">\n",
        "        {svg_polygons}\n",
        "        {svg_texture_spots}\n",
        "      </g>\n",
        "      <circle cx=\"50\" cy=\"50\" r=\"48\" fill=\"none\" stroke=\"rgba(255, 255, 255, 0.25)\" stroke-width=\"1.5\" />\n",
        "    </svg>\n",
        "    '''\n",
        "    encoded_svg = base64.b64encode(svg_string.encode('utf-8')).decode('utf-8')\n",
        "    return f\"data:image/svg+xml;base64,{encoded_svg}\"\n",
        "\n",
        "def darken_color(color_hex, factor=0.8):\n",
        "    \"\"\"Darkens or lightens a hex color by a given factor.\"\"\"\n",
        "    rgb = mcolors.to_rgb(color_hex)\n",
        "    # Clamp values to ensure they stay within the valid [0, 1] range for RGB\n",
        "    modified_rgb = [min(max(c * factor, 0), 1) for c in rgb]\n",
        "    return mcolors.to_hex(modified_rgb)\n",
        "\n",
        "def create_low_poly_sphere(center_x, center_y, center_z, radius, base_color, subdivisions=2, texture_spots=15):\n",
        "    \"\"\"\n",
        "    Generates vertex and face data for a textured low-poly sphere (icosphere).\n",
        "    \"\"\"\n",
        "    # Define the 12 vertices of a regular icosahedron\n",
        "    t = (1.0 + np.sqrt(5.0)) / 2.0\n",
        "    vertices = np.array([\n",
        "        [-1,  t,  0], [ 1,  t,  0], [-1, -t,  0], [ 1, -t,  0],\n",
        "        [ 0, -1,  t], [ 0,  1,  t], [ 0, -1, -t], [ 0,  1, -t],\n",
        "        [ t,  0, -1], [ t,  0,  1], [-t,  0, -1], [-t,  0,  1]\n",
        "    ])\n",
        "\n",
        "    # Define the 20 triangular faces of the icosahedron\n",
        "    faces = np.array([\n",
        "        [0, 11, 5], [0, 5, 1], [0, 1, 7], [0, 7, 10], [0, 10, 11],\n",
        "        [1, 5, 9], [5, 11, 4], [11, 10, 2], [10, 7, 6], [7, 1, 8],\n",
        "        [3, 9, 4], [3, 4, 2], [3, 2, 6], [3, 6, 8], [3, 8, 9],\n",
        "        [4, 9, 5], [2, 4, 11], [6, 2, 10], [8, 6, 7], [9, 8, 1]\n",
        "    ])\n",
        "\n",
        "    # Subdivide faces to create more polygons\n",
        "    for _ in range(subdivisions):\n",
        "        new_faces = []\n",
        "        mid_points = {}\n",
        "        for face in faces:\n",
        "            v_indices = [face[0], face[1], face[2]]\n",
        "            new_v_indices = []\n",
        "            for i in range(3):\n",
        "                v1 = v_indices[i]\n",
        "                v2 = v_indices[(i + 1) % 3]\n",
        "                mid_key = tuple(sorted((v1, v2)))\n",
        "                mid_idx = mid_points.get(mid_key)\n",
        "                if mid_idx is None:\n",
        "                    mid_idx = len(vertices)\n",
        "                    vertices = np.vstack([vertices, (vertices[v1] + vertices[v2]) / 2.0])\n",
        "                    mid_points[mid_key] = mid_idx\n",
        "                new_v_indices.append(mid_idx)\n",
        "            new_faces.append([v_indices[0], new_v_indices[0], new_v_indices[2]])\n",
        "            new_faces.append([v_indices[1], new_v_indices[1], new_v_indices[0]])\n",
        "            new_faces.append([v_indices[2], new_v_indices[2], new_v_indices[1]])\n",
        "            new_faces.append(new_v_indices)\n",
        "        faces = np.array(new_faces)\n",
        "\n",
        "    # Normalize vertices to form a sphere, then scale and translate\n",
        "    vertices = vertices / np.linalg.norm(vertices, axis=1)[:, np.newaxis]\n",
        "    final_vertices = vertices * radius + np.array([center_x, center_y, center_z])\n",
        "\n",
        "    # Create vertex colors for texture spots\n",
        "    darker_color_hex = darken_color(base_color, 0.7)\n",
        "    vertex_colors = [base_color] * len(final_vertices)\n",
        "    if texture_spots > 0:\n",
        "        spot_indices = np.random.choice(len(final_vertices), size=texture_spots, replace=False)\n",
        "        for idx in spot_indices:\n",
        "            vertex_colors[idx] = darker_color_hex\n",
        "\n",
        "    return final_vertices, faces, vertex_colors\n",
        "\n",
        "# --- Define New Color Palette and Colormap ---\n",
        "# Define the red and green color spectrums\n",
        "RED_SPECTRUM = {'light': '#ee868f', 'dark': '#cd2d35'}\n",
        "GREEN_SPECTRUM = {'light': '#8ac08b', 'dark': '#329542'}\n",
        "\n",
        "# Create separate colormaps for negative (red) and positive (green) values\n",
        "red_cmap = mcolors.LinearSegmentedColormap.from_list('red_cmap', [RED_SPECTRUM['light'], RED_SPECTRUM['dark']])\n",
        "green_cmap = mcolors.LinearSegmentedColormap.from_list('green_cmap', [GREEN_SPECTRUM['light'], GREEN_SPECTRUM['dark']])\n",
        "\n",
        "def get_node_color(value, min_val, max_val):\n",
        "    \"\"\"\n",
        "    Determines the node color based on its value, using a diverging red/green scale.\n",
        "    \"\"\"\n",
        "    if value >= 0:\n",
        "        if value >= max_val:\n",
        "            return GREEN_SPECTRUM['dark']\n",
        "        norm_val = value / max_val if max_val != 0 else 0\n",
        "        return mcolors.to_hex(green_cmap(norm_val))\n",
        "    else: # value < 0\n",
        "        if value <= min_val:\n",
        "            return RED_SPECTRUM['dark']\n",
        "        norm_val = value / min_val if min_val != 0 else 0\n",
        "        return mcolors.to_hex(red_cmap(norm_val))\n",
        "\n",
        "\n",
        "# --- Main Plotting Function ---\n",
        "def solar_system_visual(source_ticker, processed_data_df, source_data_df, screener_data_df, zoom=1.5):\n",
        "    \"\"\"\n",
        "    Creates the main 3D solar system visualization with a low-poly aesthetic.\n",
        "    \"\"\"\n",
        "    ticker_connections = processed_data_df[processed_data_df['source'] == source_ticker].copy()\n",
        "    source_info_row = source_data_df[source_data_df['ticker'] == source_ticker]\n",
        "\n",
        "    if ticker_connections.empty or source_info_row.empty:\n",
        "        return go.Figure().update_layout(title=f\"Data not available for {source_ticker}\", title_x=0.5, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', font_color='white')\n",
        "\n",
        "    source_info = source_info_row.iloc[0]\n",
        "    fig = go.Figure()\n",
        "\n",
        "    pos = {source_ticker: (0, 0, 0)}\n",
        "    actual_target_connections = ticker_connections[ticker_connections['target'] != source_ticker].copy()\n",
        "    num_connections = len(actual_target_connections)\n",
        "    radii_for_rings = []\n",
        "\n",
        "    if num_connections > 0:\n",
        "        original_radii = actual_target_connections['Orbital Radius']\n",
        "        min_rad, max_rad = original_radii.min(), original_radii.max()\n",
        "        rad_range = max_rad - min_rad if max_rad > min_rad else 1.0\n",
        "        min_visual_radius, max_visual_radius = 3.0, 10.0\n",
        "        visual_range = max_visual_radius - min_visual_radius\n",
        "        thetas = np.linspace(0, 2 * np.pi, num_connections, endpoint=False)\n",
        "\n",
        "        for i, (index, row) in enumerate(actual_target_connections.iterrows()):\n",
        "            scaled_radius = ((row['Orbital Radius'] - min_rad) / rad_range) * visual_range + min_visual_radius\n",
        "            radii_for_rings.append(scaled_radius)\n",
        "            theta = thetas[i]\n",
        "            pos[row['target']] = (scaled_radius * np.cos(theta), scaled_radius * np.sin(theta), 0)\n",
        "\n",
        "    # --- Add Orbital Rings ---\n",
        "    for r in sorted(list(set(radii_for_rings))):\n",
        "        theta_ring = np.linspace(0, 2 * np.pi, 100)\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=r * np.cos(theta_ring), y=r * np.sin(theta_ring), z=np.zeros(100),\n",
        "            mode='lines',\n",
        "            line=dict(color='rgba(255, 255, 255, 0.2)', width=1, dash='solid'),\n",
        "            hoverinfo='none'\n",
        "        ))\n",
        "\n",
        "    scene_annotations = []\n",
        "\n",
        "    # --- Loop Through Each Node to Draw Them ---\n",
        "    for node_name, coords in pos.items():\n",
        "        center_x, center_y, center_z = coords\n",
        "        is_source = (node_name == source_ticker)\n",
        "\n",
        "        screener_info_row = screener_data_df[screener_data_df['code'] == node_name]\n",
        "        if screener_info_row.empty: continue\n",
        "        screener_info = screener_info_row.iloc[0]\n",
        "\n",
        "        market_cap = screener_info.get('market_capitalization', 0)\n",
        "        market_cap_str = f\"${market_cap/1e12:.2f}T\" if market_cap > 1e12 else f\"${market_cap/1e9:.2f}B\"\n",
        "        min_visual_size, max_visual_size = 0.6, 1.5\n",
        "\n",
        "        if is_source:\n",
        "            hover_text = (f\"<b>{screener_info.get('name', node_name)} ({node_name})</b><br>\"\n",
        "                          f\"Industry: {screener_info.get('industry', 'N/A')}<br>\"\n",
        "                          f\"Sector: {screener_info.get('sector', 'N/A')}<br>\"\n",
        "                          f\"Avg Volume (1d): {screener_info.get('avgvol_1d', 'N/A')}<br>\"\n",
        "                          f\"Market Cap: {market_cap_str}\")\n",
        "            node_color = get_node_color(source_info['gravitational_impact'], -80, 80)\n",
        "            radius = min_visual_size + (source_info['source_planet_radius'] * (max_visual_size - min_visual_size))\n",
        "            subdivisions = 2\n",
        "        else:\n",
        "            processed_info = ticker_connections[ticker_connections['target'] == node_name].iloc[0]\n",
        "            hover_text = (f\"<b>{screener_info.get('name', node_name)} ({node_name})</b><br>\"\n",
        "                          f\"Industry: {screener_info.get('industry', 'N/A')}<br>\"\n",
        "                          f\"Sector: {screener_info.get('sector', 'N/A')}<br>\"\n",
        "                          f\"Avg Volume (1d): {screener_info.get('avgvol_1d', 'N/A')}<br>\"\n",
        "                          f\"Daily Change: {processed_info['Daily Change']:.2f}%<br>\"\n",
        "                          f\"Market Cap: {market_cap_str}\")\n",
        "            node_color = get_node_color(processed_info['Daily Change'], -5, 5)\n",
        "            radius = min_visual_size + (processed_info['Planet Radius'] * (max_visual_size - min_visual_size))\n",
        "            subdivisions = 2\n",
        "\n",
        "        vertices, faces, vertex_colors = create_low_poly_sphere(center_x, center_y, center_z, radius, node_color, subdivisions, texture_spots=15)\n",
        "        fig.add_trace(go.Mesh3d(\n",
        "            x=vertices[:, 0], y=vertices[:, 1], z=vertices[:, 2],\n",
        "            i=faces[:, 0], j=faces[:, 1], k=faces[:, 2],\n",
        "            vertexcolor=vertex_colors,\n",
        "            opacity=1.0,\n",
        "            flatshading=True,\n",
        "            # Add lighting properties to make edges crisp\n",
        "            lighting=dict(\n",
        "                ambient=0.8,\n",
        "                diffuse=0.5,\n",
        "                specular=1,\n",
        "                roughness=1\n",
        "            ),\n",
        "            hoverinfo='text',\n",
        "            text=hover_text,\n",
        "            hoverlabel=dict(bgcolor='#0f0524', font=dict(color='#EAEAEA', size=14), bordercolor='rgba(255, 255, 255, 0.3)')\n",
        "        ))\n",
        "\n",
        "        # Add labels as scene annotations\n",
        "        scene_annotations.append(\n",
        "            dict(\n",
        "                x=center_x,\n",
        "                y=center_y,\n",
        "                z=center_z, # Position label in the center of the sphere\n",
        "                text=f\"<b>{node_name}</b>\",\n",
        "                showarrow=False,\n",
        "                font=dict(color='white', size=14),\n",
        "                bgcolor=\"rgba(0,0,0,0)\",\n",
        "                xanchor=\"center\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    # --- Configure Final Layout ---\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis=dict(visible=False), yaxis=dict(visible=False), zaxis=dict(visible=False),\n",
        "            camera=dict(eye=dict(x=0.9 * zoom, y=0.9 * zoom, z=0.9 * zoom)),\n",
        "            aspectmode='data',\n",
        "            annotations=scene_annotations,\n",
        "            bgcolor='rgba(0,0,0,0)'\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=0),\n",
        "        showlegend=False,\n",
        "        paper_bgcolor='rgba(0,0,0,0)',\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        font=dict(family=\"'Space Grotesk', sans-serif\", color='#EAEAEA', size=16)\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "# --- Dash App Definition & Styling ---\n",
        "app = Dash(__name__, external_stylesheets=['https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;700&display=swap'])\n",
        "server = app.server\n",
        "\n",
        "# --- New Theme Styles ---\n",
        "THEME = {\n",
        "    'background': '#0B041A',\n",
        "    'text': '#EAEAEA',\n",
        "    'primary': '#FFFFFF',\n",
        "    'container_bg': 'rgba(30, 15, 60, 0.3)',\n",
        "    'container_border': 'rgba(255, 255, 255, 0.1)'\n",
        "}\n",
        "\n",
        "# Starry background style\n",
        "starry_background_style = {\n",
        "    'backgroundColor': THEME['background'],\n",
        "    'backgroundImage': 'radial-gradient(circle, white 0.5px, transparent 1.5px), radial-gradient(circle, white 1px, transparent 2px), radial-gradient(circle, white 0.5px, transparent 1.5px)',\n",
        "    'backgroundSize': '350px 350px, 250px 250px, 150px 150px',\n",
        "    'backgroundPosition': '0 0, 40px 60px, 130px 270px',\n",
        "    'color': THEME['text'],\n",
        "    'fontFamily': \"'Space Grotesk', sans-serif\",\n",
        "    'minHeight': '100vh',\n",
        "    'padding': '20px'\n",
        "}\n",
        "\n",
        "font_style = {'fontFamily': \"'Space Grotesk', sans-serif\", 'color': THEME['text']}\n",
        "container_style = {\n",
        "    'backgroundColor': THEME['container_bg'],\n",
        "    'border': f\"1px solid {THEME['container_border']}\",\n",
        "    'padding': '25px 30px',\n",
        "    'borderRadius': '12px',\n",
        "    'backdropFilter': 'blur(10px)',\n",
        "    'width': '100%',\n",
        "    'boxSizing': 'border-box'\n",
        "}\n",
        "header_style = {**font_style, 'color': THEME['text'], 'textAlign': 'center', 'fontWeight': 'bold', 'marginTop': 0, 'marginBottom': '20px', 'fontSize': '22px'}\n",
        "\n",
        "# Assume dataframes are loaded\n",
        "ticker_options = [{'label': row['name'] + f\" ({row['code']})\", 'value': row['code']} for index, row in screener_data_df.iterrows()]\n",
        "default_ticker = 'AAPL'\n",
        "\n",
        "# --- App Layout ---\n",
        "app.layout = html.Div(style=starry_background_style, children=[\n",
        "    dcc.Store(id='zoom-level-store', data=1.5), # Store for zoom level\n",
        "    html.H1(\"THE FINANCIAL OBSERVATORY\", style={**font_style, 'textAlign': 'center', 'padding': '20px 0', 'fontSize': '32px', 'fontWeight': 'bold', 'letterSpacing': '4px'}),\n",
        "\n",
        "    html.Div([\n",
        "        dcc.Dropdown(id='ticker-dropdown', options=ticker_options, value=default_ticker, clearable=False, style={'height': '40px', 'color': 'black'})\n",
        "    ], style={'width': '90%', 'maxWidth': '500px', 'margin': '0 auto 20px auto', 'backgroundColor': THEME['container_bg'], 'border': f\"1px solid {THEME['container_border']}\", 'borderRadius': '12px', 'backdropFilter': 'blur(10px)', 'padding': '5px'}),\n",
        "\n",
        "    html.P(id='prediction-summary-text', style={'textAlign': 'center', 'padding': '10px 0', 'fontSize': '18px', 'color': THEME['text']}),\n",
        "\n",
        "    html.Div(id='graph-container', style={'height': '50vh', 'width': '98%', 'margin': 'auto', 'borderRadius': '15px', 'boxShadow': '0 0 25px 5px rgba(255, 255, 255, 0.15)'}),\n",
        "\n",
        "    # Zoom slider container\n",
        "    html.Div([\n",
        "        dcc.Slider(\n",
        "            id='zoom-slider',\n",
        "            min=0.5,\n",
        "            max=3,\n",
        "            step=0.1,\n",
        "            value=1.5,\n",
        "            marks={\n",
        "                0.5: {'label': 'Zoom In', 'style': {'color': 'white', 'fontWeight': 'bold'}},\n",
        "                3: {'label': 'Zoom Out', 'style': {'color': 'white', 'fontWeight': 'bold'}}\n",
        "            },\n",
        "            className='themed-slider'\n",
        "        )\n",
        "    ], style={'width': '80%', 'maxWidth': '600px', 'margin': '30px auto 20px auto'}),\n",
        "\n",
        "    html.Div(id='info-panels-container')\n",
        "])\n",
        "\n",
        "\n",
        "# --- Callbacks ---\n",
        "\n",
        "# Combined callback for updating data and panels when ticker changes\n",
        "@app.callback(\n",
        "    [Output('info-panels-container', 'children'),\n",
        "     Output('prediction-summary-text', 'children'),\n",
        "     Output('graph-container', 'children')],\n",
        "    [Input('ticker-dropdown', 'value')],\n",
        "    [State('zoom-level-store', 'data')]\n",
        ")\n",
        "def update_content_on_ticker_change(selected_ticker, zoom_level):\n",
        "\n",
        "    # Determine if it's the weekend\n",
        "    today_weekday = datetime.today().weekday()\n",
        "    is_weekend = today_weekday >= 5  # Saturday is 5, Sunday is 6\n",
        "\n",
        "    global processed_data_df, source_data_df\n",
        "    processed_data_df, source_data_df = process_and_score_stocks(\n",
        "        six_month_spearman_lagged_correlations, three_month_spearman_lagged_correlations, screener_data_df,\n",
        "        selected_ticker, min_nodes, max_nodes, threshold_percent\n",
        "    )\n",
        "\n",
        "    if processed_data_df.empty or source_data_df.empty:\n",
        "        empty_fig = go.Figure().update_layout(title=f\"Data not available for {selected_ticker}\", title_x=0.5, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', font_color='white')\n",
        "        empty_graph = dcc.Graph(id='network-graph', figure=empty_fig, style={'height': '100%'})\n",
        "        no_data_msg = html.Div(f\"Data not available for {selected_ticker}\", style={'textAlign': 'center', 'padding': '20px'})\n",
        "        return no_data_msg, \"\", empty_graph\n",
        "\n",
        "    # Regenerate graph with the stored zoom level\n",
        "    graph = dcc.Graph(id='network-graph', figure=solar_system_visual(selected_ticker, processed_data_df, source_data_df, screener_data_df, zoom_level), style={'height': '100%'})\n",
        "\n",
        "    # --- Generate data for panels and summary text ---\n",
        "    star_info_screener = screener_data_df[screener_data_df['code'] == selected_ticker].iloc[0]\n",
        "    star_info_source = source_data_df[source_data_df['ticker'] == selected_ticker].iloc[0]\n",
        "    grav_impact = star_info_source.get('gravitational_impact', 0)\n",
        "    net_gravitational_force = star_info_source.get('net_gravitational_force', 0)\n",
        "    max_potential_force = star_info_source.get('max_potential_force', 0)\n",
        "\n",
        "    planets_df = processed_data_df[processed_data_df['source'] == selected_ticker].copy()\n",
        "    max_gravitational_force = planets_df['signed_gravitational_force'].abs().sum()\n",
        "\n",
        "    # --- Conditional text based on day of the week ---\n",
        "    prediction_day_text = \"on Monday\" if is_weekend else \"today\"\n",
        "    daily_change_header = [\"Friday's\", html.Br(), \"Daily Close\"] if is_weekend else [\"Yesterday's\", html.Br(), \"Daily Change\"]\n",
        "    predictions_header_text = \"Monday's Top Predictions\" if is_weekend else \"Top Predictions\"\n",
        "\n",
        "\n",
        "    # Summary text\n",
        "    source_name = star_info_screener.get('name', selected_ticker)\n",
        "    direction = \"increase\" if grav_impact >= 0 else \"decrease\"\n",
        "    strength = grav_impact\n",
        "    prediction_summary = f\"{source_name} ({selected_ticker}) is predicted to {direction} {prediction_day_text} with a prediction strength of {strength:.2f}%.\"\n",
        "\n",
        "    # Star Info Panel\n",
        "    star_color = get_node_color(grav_impact, -80, 80)\n",
        "    star_image_src = create_model_image_svg(star_color, subdivisions=2, texture_spots_count=10)\n",
        "\n",
        "    star_text_content = html.Div([\n",
        "        html.P(\n",
        "            f\"{source_name} ({selected_ticker}) is expected to {direction} {prediction_day_text} with a prediction strength of {grav_impact:.2f}%. \"\n",
        "            f\"The prediction strength is calculated based on how correlated stocks are with the next day performance of {source_name} ({selected_ticker}).\"\n",
        "        ),\n",
        "        html.P(\n",
        "            f\"If all of the planets in the solar system had perfect correlations, the maximum gravitational force that could exist in the system would be {max_potential_force:.2f}. \"\n",
        "            f\"The net gravitational force acting on the star right now is {net_gravitational_force:.2f}.\",\n",
        "            style={'marginTop': '10px'}\n",
        "        )\n",
        "    ], style={\n",
        "        'maxHeight': '120px',\n",
        "        'overflowY': 'auto',\n",
        "        'paddingRight': '15px',\n",
        "        'maskImage': 'linear-gradient(to bottom, black 60%, transparent 100%)',\n",
        "        'WebkitMaskImage': 'linear-gradient(to bottom, black 60%, transparent 100%)'\n",
        "    })\n",
        "\n",
        "    star_info_panel = html.Div([\n",
        "        html.H3(\"Star Information\", style=header_style),\n",
        "        html.Div([\n",
        "            html.Img(src=star_image_src, style={'height': '100px', 'width': '100px', 'marginRight': '20px', 'flexShrink': '0'}),\n",
        "            star_text_content\n",
        "        ], style={'display': 'flex', 'alignItems': 'center'})\n",
        "    ], style=container_style)\n",
        "\n",
        "    # --- Predictions Panel ---\n",
        "    prediction_items = []\n",
        "    if 'top_positive_impacts' in globals() and not top_positive_impacts.empty and 'top_negative_impacts' in globals() and not top_negative_impacts.empty:\n",
        "        combined_impacts = pd.concat([top_positive_impacts.head(5), top_negative_impacts.head(5)])\n",
        "\n",
        "        def create_prediction_item(row):\n",
        "            ticker = row['ticker']\n",
        "            screener_row = screener_data_df[screener_data_df['code'] == ticker]\n",
        "            name = screener_row['name'].iloc[0] if not screener_row.empty else ticker\n",
        "            display_text = f\"{name} ({ticker})\"\n",
        "            return html.Div([\n",
        "                html.Span(display_text),\n",
        "                html.Span(f\"{row['gravitational_impact']:.2f}%\", style={'color': '#4ade80' if row['gravitational_impact'] > 0 else '#f87171', 'fontWeight': 'bold'})\n",
        "            ],\n",
        "            id={'type': 'prediction-item', 'index': ticker},\n",
        "            n_clicks=0,\n",
        "            style={'display': 'flex', 'justifyContent': 'space-between', 'padding': '8px 0', 'borderBottom': f'1px solid {THEME[\"container_border\"]}', 'cursor': 'pointer'}\n",
        "            )\n",
        "\n",
        "        prediction_items = [create_prediction_item(row) for _, row in combined_impacts.iterrows()]\n",
        "\n",
        "    else:\n",
        "         prediction_items = [html.Div(\"Top predictions not available.\")]\n",
        "\n",
        "    predictions_panel = html.Div([html.H3(predictions_header_text, style=header_style), html.Div(prediction_items)], style=container_style)\n",
        "\n",
        "\n",
        "    # --- Legend Panel ---\n",
        "    legend_gradient_style = {'height': '15px', 'width': '100%', 'maxWidth': '300px', 'borderRadius': '5px', 'marginBottom': '8px', 'background': f\"linear-gradient(to right, {RED_SPECTRUM['dark']}, {RED_SPECTRUM['light']} 49.9%, {GREEN_SPECTRUM['light']} 50.1%, {GREEN_SPECTRUM['dark']})\"}\n",
        "    legend_content = html.Div([\n",
        "        html.Div(style=legend_gradient_style),\n",
        "        html.Div([html.Span(\"Decrease\"), html.Span(\"Increase\")], style={'display': 'flex', 'justifyContent': 'space-between', 'fontSize': '12px', 'width': '100%', 'maxWidth': '300px'})\n",
        "    ], style={'display': 'flex', 'flexDirection': 'column', 'alignItems': 'center'})\n",
        "    legend_panel_container = html.Div([\n",
        "        html.H3(\"Legend\", style={**header_style, 'marginBottom': '15px', 'textAlign': 'center'}),\n",
        "        legend_content\n",
        "    ], style={**container_style, 'padding': '15px 20px', 'width': '100%', 'align-self': 'center'})\n",
        "\n",
        "    # --- Planet Info Table ---\n",
        "    headers = [\n",
        "        \"Code\",\n",
        "        \"Name\",\n",
        "        [\"Correlation\", html.Br(), f\"with {selected_ticker}\"],\n",
        "        \"Market Cap\",\n",
        "        daily_change_header,\n",
        "        \"Grav. Force\"\n",
        "    ]\n",
        "    table_header = [html.Thead(html.Tr([html.Th(col, style={'padding': '12px', 'textAlign': 'left', 'borderBottom': f\"2px solid {THEME['container_border']}\"}) for col in headers]))]\n",
        "\n",
        "    table_rows = []\n",
        "    if not planets_df.empty:\n",
        "        for _, p_row in planets_df.iterrows():\n",
        "            s_info = screener_data_df[screener_data_df['code'] == p_row['target']].iloc[0]\n",
        "            planet_color = get_node_color(p_row['Daily Change'], -5, 5)\n",
        "            planet_image_src = create_model_image_svg(planet_color, subdivisions=2, texture_spots_count=5)\n",
        "\n",
        "            ticker_cell = html.Div([\n",
        "                html.Img(src=planet_image_src, style={'height': '40px', 'width': '40px', 'marginRight': '10px'}),\n",
        "                html.Span(p_row['target'])\n",
        "            ], style={'display': 'flex', 'alignItems': 'center'})\n",
        "\n",
        "            table_rows.append(html.Tr([\n",
        "                html.Td(ticker_cell, style={'padding': '8px 12px', 'borderBottom': f'1px solid {THEME[\"container_border\"]}'}),\n",
        "                html.Td(s_info['name'], style={'padding': '8px 12px', 'borderBottom': f'1px solid {THEME[\"container_border\"]}'}),\n",
        "                html.Td(f\"{p_row['unified_correlation']:.2%}\", style={'padding': '8px 12px', 'borderBottom': f'1px solid {THEME[\"container_border\"]}'}),\n",
        "                html.Td(f\"${s_info['market_capitalization']/1e9:.2f}B\", style={'padding': '8px 12px', 'borderBottom': f'1px solid {THEME[\"container_border\"]}'}),\n",
        "                html.Td(f\"{p_row['Daily Change']:.2f}%\", style={'padding': '8px 12px', 'borderBottom': f'1px solid {THEME[\"container_border\"]}'}),\n",
        "                html.Td(f\"{p_row['signed_gravitational_force']:.2f}\", style={'padding': '8px 12px', 'borderBottom': f'1px solid {THEME[\"container_border\"]}'}),\n",
        "            ]))\n",
        "\n",
        "    # Dynamically set the height of the table container\n",
        "    table_wrapper_style = {\n",
        "        'overflowX': 'auto',\n",
        "        'maskImage': 'linear-gradient(to right, black 95%, transparent 100%)',\n",
        "        'WebkitMaskImage': 'linear-gradient(to right, black 95%, transparent 100%)'\n",
        "    }\n",
        "    if len(planets_df) > 5:\n",
        "        table_wrapper_style['maxHeight'] = '300px'\n",
        "        table_wrapper_style['overflowY'] = 'auto'\n",
        "\n",
        "    planet_table_panel = html.Div([\n",
        "        html.H3(\"Planet Information\", style=header_style),\n",
        "        html.Div(html.Table(table_header + [html.Tbody(table_rows)], style={'width': '100%', 'borderCollapse': 'collapse'}), style=table_wrapper_style)\n",
        "    ], style=container_style)\n",
        "\n",
        "    # Assemble the info panels layout\n",
        "    info_panels = html.Div([\n",
        "        legend_panel_container,\n",
        "        star_info_panel,\n",
        "        planet_table_panel,\n",
        "        predictions_panel\n",
        "    ], style={'display': 'flex', 'flexDirection': 'column', 'gap': '20px', 'padding': '20px 0'})\n",
        "\n",
        "    return info_panels, prediction_summary, graph\n",
        "\n",
        "# Callback to update zoom level in store\n",
        "@app.callback(\n",
        "    Output('zoom-level-store', 'data'),\n",
        "    Input('zoom-slider', 'value')\n",
        ")\n",
        "def update_zoom_store(zoom_level):\n",
        "    return zoom_level\n",
        "\n",
        "# Callback to update graph only when zoom level changes\n",
        "@app.callback(\n",
        "    Output('graph-container', 'children', allow_duplicate=True),\n",
        "    Input('zoom-level-store', 'data'),\n",
        "    State('ticker-dropdown', 'value'),\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def update_graph_on_zoom(zoom_level, selected_ticker):\n",
        "    if selected_ticker is None:\n",
        "        raise PreventUpdate\n",
        "\n",
        "    # We need the dataframes, so we might have to re-run the processing if they are not available globally\n",
        "    # For this example, we assume they exist from the last ticker change\n",
        "    if 'processed_data_df' not in globals() or 'source_data_df' not in globals():\n",
        "        raise PreventUpdate\n",
        "\n",
        "    figure = solar_system_visual(selected_ticker, processed_data_df, source_data_df, screener_data_df, zoom_level)\n",
        "    return dcc.Graph(id='network-graph', figure=figure, style={'height': '100%'})\n",
        "\n",
        "# Callback to update dropdown when a prediction is clicked\n",
        "@app.callback(\n",
        "    Output('ticker-dropdown', 'value'),\n",
        "    Input({'type': 'prediction-item', 'index': ALL}, 'n_clicks'),\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def update_dropdown_from_prediction_click(n_clicks):\n",
        "    if not any(n_clicks):\n",
        "        raise PreventUpdate\n",
        "\n",
        "    ctx = callback_context\n",
        "    if not ctx.triggered:\n",
        "        raise PreventUpdate\n",
        "\n",
        "    triggered_id_str = ctx.triggered[0]['prop_id'].split('.')[0]\n",
        "\n",
        "    if not triggered_id_str:\n",
        "        raise PreventUpdate\n",
        "\n",
        "    triggered_id = json.loads(triggered_id_str)\n",
        "\n",
        "    new_ticker = triggered_id['index']\n",
        "\n",
        "    return new_ticker\n",
        "\n",
        "# --- Run the App ---\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, port=8051)\n"
      ],
      "metadata": {
        "id": "g50i80RE6CwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}